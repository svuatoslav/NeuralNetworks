{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные этапы работы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 0. Установка и настройка оболочки для работы с языком Python\n",
    "\n",
    "В лабораторной работе использовалсь среда разработки VS Code с расширением Juputer <u>version: 2024.2.0</u>.\n",
    "\n",
    "Загрузка необходимых библиотек для выполнения лаборатрных работ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лабораторной работе используется модели обучения Random Forest, которые предоствалены в библиотеке sklearn. \n",
    "\n",
    "Scikit-learn (sklearn) — это один из наиболее широко используемых пакетов Python для Data Science и Machine Learning.\n",
    "\n",
    "Random Forest (случайный лес) – это алгоритм машинного обучения, который используется для решения задач классификации и регрессии. Он является расширением алгоритма решающих деревьев, который использует ансамбль деревьев для улучшения качества классификации или регрессии. Суть алгоритма заключается в том, что он создает множество решающих деревьев и использует их для предсказания классов объектов. Каждое дерево строится на случайном подмножестве обучающих данных и случайном подмножестве признаков. В результате, каждое дерево в ансамбле получается немного разным, что позволяет уменьшить эффект переобучения и повысить качество предсказаний.\n",
    "\n",
    "Для работы с генетическими алгоритмами используется каркас DEAP – мощным и  гибким каркасом эволюционных вычислений для решения практических задач с помощью генетических алгоритмов. DEAP (сокращение от Distributed Evolutionary Algorithms in Python – распределенные эволюционные алгоритмы на Python) поддерживает быструю разработку решений с применением генетических алгоритмов и других методов эволюционных вычислений. DEAP предлагает различные структуры данных и инструменты, необходимые для реализации самых разных решений на основе генетических алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 1. Построение бинарного классификатора.\n",
    "**Целью этапа:** является создание бинарного классификатора отзывов к фильмам из наборы данных IMDB, использовав для улучшения качества модели генетические алгоритмы. \n",
    "\n",
    "**Формулировка задания:** с помощью генетического алгоритма улучшить качество модели машинного обучения с учителем RandomForestClassifier за счёт настройки его гиперпараметров; классифицировать отзывы к фильмам на положительные и отрицательные отзывы, опираясь на текст отзывов, использовав классификатор RandomForestClassifier.\n",
    "\n",
    "Загрузка набора данных IMDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "База данных состоит из 50000 отзывов к кинолентам в интернет-базе (Internet Movie Database). Набор разбит на 25000 обучающих и 25 000 контрольных отзывов, каждый набор на 50 % состоит из отрицательных и на 50 % из положительных отзывов. Набор данных IMDB поставляется в составе Keras. Набор готов к использованию: отзывы\n",
    "(последовательности слов) преобразованы в последовательности целых чисел, каждое из которых определяет позицию слова в словаре.\t\n",
    "\t\n",
    "Подготовим константы и диапазоны значений для работы с генетическим алгоритмом: \n",
    "\n",
    "В модели _RandomForestClassifier_ существует множество гиперпараметров, в лабораторной работе мы используем следющие гиперпараметры:\n",
    "1. n_estimators – количество деревьев в лесу.\n",
    "2. criterion - функция для измерения качества разделения. Поддерживаемые критерии: gini для примеси Джини и ”log_loss“ и \"entropy\" для усиления информации по Шеннону.\n",
    "3. max_depth - максимальная глубина дерева. Если None, то узлы расширяются до тех пор, пока все листья не станут чистыми или пока все листья не будут содержать меньше выборок min_samples_split.\n",
    "4. min_samples_split - минимальное число выборок, для разделения внутреннего узла.\n",
    "5. min_samples_leaf - минимальное число образцов должны быть на листе. Точка разделения на любой глубине будет рассматриваться только в том случае, если она оставляет не менее min_samples_leaf образцов подготовки в каждой из левой и правой ветвей. Это может привести к сглаживанию модели, особенно при регрессии.\n",
    "6. max_features - количество функций, которые следует учитывать при поиске наилучшего разделения: если “sqrt”, то max_features=sqrt(n_features), если “log2”, то max_features=log2(n_features), если None, то max_features=n_features, где n_features это количество особенностей, наблюдаемых во время подгонки (fit).\n",
    "7. max_leaf_nodes - выращивайте деревья с помощью max_leaf_nodes по принципу \"сначала лучше\". Лучшие узлы определяются как относительное уменьшение примесей. Если None, то неограниченное количество конечных узлов.\n",
    "8. bootstrap - используются ли выборки bootstrap при построении деревьев. Если значение равно False, для построения каждого дерева используется весь набор данных.\n",
    "9. class_weight - веса, связанные с классами в форме {class_label: weight}. Если None, предполагается, что все классы имеют вес один. Для задач с несколькими выводами список dicts может быть предоставлен в том же порядке, что и столбцы y. Режим “balanced” использует значения y для автоматической настройки весов, обратно пропорциональных частотам классов во входных данных, как n_samples / (n_classes * np.bincount(y)). Режим “balanced_subsample” такой же, как и “balanced”, за исключением того, что веса вычисляются на основе начальной выборки для каждого выращенного дерева.\n",
    "10. ccp_alpha - параметр сложности, используемый для сокращения сложности с минимальными затратами. Будет выбрано поддерево с наибольшей сложностью затрат, которое меньше ccp_alpha. По умолчанию обрезка не выполняется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список гиперпараметрв и их допустимы значения:\n",
    "# n_estimators: int, \n",
    "# criterion: string, \n",
    "# max_depth: int_None, \n",
    "# min_samples_split: int,\n",
    "# min_samples_leaf: int,\n",
    "# max_features: string_None, \n",
    "# max_leaf_nodes: int_None,\n",
    "# bootstrap: bool, \n",
    "# class_weight: string_None, \n",
    "# ccp_alpha: non-negative float\n",
    "BOUNDS_LOW =  [10, 0, 0, 2, 2, 0, 2, 0, 0, 0.0]\n",
    "BOUNDS_HIGH = [200, 2, 20, 10, 10, 2, 10, 1, 2, 0.05]\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Константы генетического алгоритма:\n",
    "POPULATION_SIZE = 20\n",
    "P_CROSSOVER = 0.9\n",
    "P_MUTATION = 0.2 \n",
    "MAX_GENERATIONS = 10\n",
    "HALL_OF_FAME_SIZE = 5\n",
    "CROWDING_FACTOR = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_BOUNDS_LOW_ — массив содержащий нижние границы значений гиперпараметров.\n",
    "\n",
    "_BOUNDS_HIGH_ — массив содержащий верхние границы значений гиперпараметров.\n",
    "\n",
    "Порядок гиперпараметров в массивах _BOUNDS_LOW_ и _BOUNDS_HIGH_:\n",
    "\n",
    "_[n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, class_weight, ccp_alpha]_.\n",
    "\n",
    "_POPULATION_SIZE_ — количество индивидуумов в популяции\n",
    "\n",
    "_P_CROSSOVER_ = 0.9 — вероятность скрещивания\n",
    "\n",
    "_P_MUTATION_ = 0.2 — вероятность мутации индивидуума\n",
    "\n",
    "_MAX_GENERATIONS_ = 10 — максимальное количество поколений\n",
    "\n",
    "_HALL_OF_FAME_SIZE_ — количество индивидуумов, которых мы хотим хранить в зале славы.\n",
    "\n",
    "_CROWDING_FACTOR_ = 20.0 — фактор вытеснения для скрещивания и мутации.\n",
    "\n",
    "Подготовим класс для получениия оценки верности классификатора обученной на данных из IMDB: \n",
    "\n",
    "Напиишем класс _HyperparameterTuningGenetic_ для оценки верности классификатора:\n",
    "\n",
    "Метод класса _initIMDBDataset_ выгружает данные из набора данных imdb и вызывает метод _Vectorize_sequences_ для векторизации тренировочных и тестовых данных, также векторизует метки. Аргумент _num_words=10000_ означает, что в обучающих данных будет сохранено только 10000 слов, наиболее часто встречающихся в обучающем наборе отзывов, остальные слова будут отброшены.\n",
    "\n",
    "Переменные _train_data_ и _test_data_ — это списки отзывов; каждый отзыв — это список индексов слов (кодированное представление последовательности слов).\n",
    "\n",
    "Переменные _train_labels_ и _test_labels_ — это списки нулей и единиц, где нули соответствуют отрицательным отзывам, а единицы — положительным.\n",
    "\n",
    "Метод класса _Vectorize_sequences_ принимает список отзывов и выполняет над ним прямое кодирование списков в векторы нулей и единиц и возвращает 10000-мерный вектор. Работа этой функции более подробно описан в отчете первой лабораторной работы.\n",
    "\n",
    "Метод класса _convertParam_ принимает список params, содержащий значения гиперпараметров типа float, и возвращает преобразованные гиперпараметры в истинных значениях.\n",
    "\n",
    "Метод класса _getAccuracy_ принимает список чисел типа float, представляющих значения гиперпараметров, вызывает метод convertParam() для преобразования их в истинные значения и инициализирует классификатор _RandomForestClassifier_ с этими значениями. Затем он вычисляет верность классификатора.\n",
    "\n",
    "Метод класса _formatParams_ принимает список params, вызывает метод _convertParam_ для преобразования их в истинные значения, и возвращает строку, которая показывает значения гиперпараметров более информативно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuningGenetic:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initIMDBDataset()\n",
    "        \n",
    "    def initIMDBDataset(self):\n",
    "        (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "        self.x_train = self.Vectorize_sequences(train_data)\n",
    "        self.x_test = self.Vectorize_sequences(test_data)\n",
    "        self.y_train = numpy.asarray(train_labels).astype('float32')\n",
    "        self.y_test = numpy.asarray(test_labels).astype('float32')\n",
    "    \n",
    "    def Vectorize_sequences(self, sequences, dimension=10000):\n",
    "        results = numpy.zeros((len(sequences), dimension))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            results[i, sequence] = 1.\n",
    "        return results\n",
    "    \n",
    "    def convertParams(self, params):\n",
    "        n_estimators = round(params[0])\n",
    "        criterion = ['gini', 'entropy', 'log_loss'][round(params[1])]\n",
    "        max_depth = None if round(params[2]) == 0 else round(params[2])\n",
    "        min_samples_split = round(params[3])\n",
    "        min_samples_leaf = round(params[4])\n",
    "        max_features = ['sqrt', 'log2', None][round(params[5])]\n",
    "        max_leaf_nodes = None if round(params[6]) == 0 else round(params[6])\n",
    "        bootstrap = True if round(params[7]) == 1 else False\n",
    "        class_weight = ['balanced', 'balanced_subsample', None][round(params[8])]\n",
    "        ccp_alpha = params[9]\n",
    "        return n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, class_weight, ccp_alpha\n",
    "\n",
    "    def getAccuracy(self, params):\n",
    "        n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, class_weight, ccp_alpha = self.convertParams(params)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features, max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, class_weight=class_weight, ccp_alpha=ccp_alpha)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        return accuracy_score(self.y_test, y_pred)\n",
    "    \n",
    "    def formatParams(self, params):\n",
    "        return \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'max_leaf_nodes'=%r, 'bootstrap'=%r, 'class_weight'=%s, 'ccp_alpha'=%.5f\" % (self.convertParams(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каркас DEAP включает несколько встроенных эволюционных алгоритмов, находящихся в  модуле _algorithms_. Один из них, _eaSimple_, реализует общую структуру генетического алгоритма и  может заменить большую часть написанного нами кода. Для сбора и печати статистики можно использовать другие полезные объекты _DEAP_, _Statistics_ и _logbook_. У встроенного метода _algorithms.eaSimple_ есть еще одна возможность – зал славы (hall of fame, сокращенно hof). Класс _HallOfFame_, находящийся в модуле _tools_, позволяет сохранить лучших индивидуумов, встретившихся в процессе эволюции, даже если вследствие отбора, скрещивания и мутации они были в какой-то момент утрачены. Зал славы поддерживается в отсортированном состоянии, так что первым элементом всегда является индивидуум с наилучшим встретившимся значением приспособленности.\n",
    "\n",
    "Однако с помощью _eaSimple_, нам не удается сохранить лучшие решения. Поскольку средняя приспособленность популяции в генетическом алгоритме, возрастает от поколения к поколению, в любой момент может случиться так, что лучшие индивидуумы в текущем поколении исчезнут. Это связано с тем, что операторы отбора, скрещивания и мутации изменяют индивидуумов в процессе создания следующего поколения. Во многих случаях потеря временная, поскольку эти (или даже лучшие) индивидуумы снова появятся в будущем поколении. Но если мы хотим гарантировать, что лучшие индивидуумы обязательно переходят в  следующее поколение, то можем применить факультативную стратегию элитизма. Это означает, что n лучших индивидуумов (_n_  – небольшое, заранее заданное число) копируются в следующее поколение, до того как все места будут заняты потомками, полученными в результате отбора, скрещивания и мутации. Скопированные элитные индивидуумы попрежнему могут использоваться как родители новых индивидуумов. Иногда элитизм оказывает заметный положительный эффект на качество алгоритма, поскольку не нужно тратить времени на повторное открытие хороших решений, потерянных в результате эволюции.\n",
    "\n",
    "Элитизм позволяет сохранить лучшие решения, защитив их от применения операторов отбора, скрещивания и мутации. Для его реализации придется залезть под капот и модифицировать код алгоритма _DEAP algorithms.eaSimple_, поскольку каркас не дает прямого способа обойти эти операторы.\n",
    "\n",
    "Метод _eaSimpleWithElitism_ аналогичен оригинальному методу _eaSimple_, но теперь объект _halloffame_ используется для реализации механизма элитизма. Индивидуумы, хранящиеся в объекте _halloffame_, просто копируются в  следующее поколение, не подвергаясь воздействию операторов отбора, скрещивания и мутации. Для этого нужно внести следующие модификации:\n",
    "* вместо того чтобы отбирать индивидуумов в количестве, равном размеру популяции, мы отбираем их меньше на столько, сколько индивидуумов находится в зале славы:\n",
    "\n",
    "_offspring = toolbox.select(population, len(population) - hof_size)_\n",
    "\n",
    "* после применения генетических операторов индивидуумы добавляются из зала славы в популяцию:\n",
    "\n",
    "offspring.extend(halloffame.items)\n",
    "\n",
    "\n",
    "Метод _eaSimpleWithElitism_ предполагает, что в toolbox уже зарегистрированы операторы evaluate, select, mate и mutate. Условие остановки задается с помощью параметра ngen – максимального количества поколений.\n",
    "\n",
    "Метод _eaSimpleWithElitism_ возвращает два объекта – конечную популяцию и объект _logbook_, содержащий собранную статистику. Интересующую насс татистику можно извлечь методом _select()_ и использовать для построения графиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__): # элитарность\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    # Этот алгоритм аналогичен алгоритму Deeper Simple() с той модификацией, \n",
    "    # что halloffame используется для реализации механизма элитарности. \n",
    "    # Особи, содержащиеся в halloffame, напрямую передаются следующему поколению и \n",
    "    # не подвергаются генетическим операторам отбора, скрещивания и мутации\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Оценивайте \"individuals\" с \"fitness\"\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None: # параметр halloffame не должен быть пустым!\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\") \n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Начните процесс смены поколений\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Выберите людей следующего поколения\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Расширяйте круг \"individuals\"\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Оценивайте \"individuals\" с \"fitness\"\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # добавляйте лучших обратно в популяцию:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Обновите зал славы сгенерированными \"individuals\"\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Замените текущую популяцию потомством\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Добавьте статистику текущей генерации в журнал регистрации\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к написанию генетического алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем экземпляр класса HyperparameterTuningGenetic, который позволит нам проверить различные комбинации гиперпараметров:\n",
    "test = HyperparameterTuningGenetic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы используем механизм, предлагаемый каркасом DEAP, – класс base.Toolbox. Он используется как контейнер для функций (или операторов) и позволяет создавать новые операторы путем назначения псевдонимов или настройки существующих функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модуль _DEAP_ creator спользуется как метафабрика и позволяет расширять существующие классы, добавляя в них новые атрибуты.\n",
    "\n",
    "При работе с _DEAP_ значения приспособленности инкапсулированы в классе _Fitness_. _DEAP_ позволяет распределять приспособленность по нескольким компонентам (называемым целями), у каждого из которых есть свой вес. Комбинация весов определяет поведение, или стратегию приспособления в конкретной задаче.\n",
    "\n",
    "Для определения стратегии в состав _DEAP_ входит абстрактный класс _base.Fitness_, который содержит кортеж _weights_. Этому кортежу необходимо присвоить значения, чтобы определить стратегию и сделать класс пригодным для использования. Для этого мы расширяем базовый класс _Fitness_ с помощью модуля creator. Поскольку мы стремимся максимизировать верность классификатора, определим единственную цель – максимизирующую стратегию приспособления _FitnessMax_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определите единую цель, максимизирующую фитнес-стратегию:\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второе применение модуля creator – определение индивидуумов, образующих популяцию в генетическом алгоритме. В DEAP класс Individual создается путем расширения базового класса, представляющего хромосому. Кроме того, каждый экземпляр класса Individual должен содержать функцию приспособленности в качестве атрибута.\n",
    "Чтобы удовлетворить обоим требованиям, мы воспользуемся модулем creator для создания класса creator.Individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создайте индивидуальный класс на основе списка:\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданный класс Individual расширяет встроенный класс Python list. Это означает, что все хромосомы имеют тип list; в каждом экземпляре класса Individual имеется атрибут fitness созданного ранее класса FitnessMax.\n",
    "\n",
    "Решение представлено списком чисел типа _float_, принадлежащих различным диапазонам, мы в цикле обойдем все пары, состоящие из нижней и верхней границ, и для каждого гиперпараметра создадим в инструментарии свой оператор, который будет генерировать случайное число в соответствующем диапазоне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определите атрибуты гиперпараметра индивидуально:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"hyperparameter_0\", \"hyperparameter_1\", ...\n",
    "    toolbox.register(\"hyperparameter_\" + str(i),\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем кортеж hyperparameters, содержащий только что созданные генераторы случайных чисел для отдельных гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создайте кортеж, содержащий генератор атрибутов для каждого искомого параметра:\n",
    "hyperparameters = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    hyperparameters = hyperparameters + \\\n",
    "                      (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать этот кортеж в сочетании со встроенным в _DEAP_ оператором _initCycle()_, чтобы создать новый оператор _individualCreator_, который инициализирует индивидуум комбинацией случайных значений гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создайте отдельный оператор для заполнения Individual экземпляра:\n",
    "toolbox.register(\"individualCreator\",# исправить\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual,\n",
    "                 hyperparameters,\n",
    "                 n=1)\n",
    "\n",
    "# создайте оператора population для создания списка individual:\n",
    "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.individualCreator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод _getAccuracy()_ экземпляра _HyperparameterTuningGenetic_ для вычисления приспособленности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitness calculation\n",
    "def classificationAccuracy(individual):\n",
    "    return test.getAccuracy(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", classificationAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих случаях класс Toolbox используется для настройки существующихфункций из модуля tools, который содержит ряд полезных функций, относящихся к генетическим операциям отбора, скрещивания и мутации, а также утилиты для инициализации.\n",
    "\n",
    "Определим генетические операторы. В качестве оператора отбора используем турнир размера 2, а для скрещивания и мутации возьмем операторы, специализированные для хромосом в виде списков чисел с плавающей точкой, ограниченных предельными значениями гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генетические операторы:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2) # турнирный отбор = 2\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,# ограниченный вариант оператора cxSimulatedBinary(), принимающий аргументы low и up – нижнюю и верхнюю границы области поиска соответственно\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0 / NUM_OF_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем подробнее:\n",
    "* Имя select зарегистрировано как псевдоним существующей в модуле tools функции selTournament() с аргументом tournsize = 2. В результате создается оператор toolbox.select, который выполняет турнирный отбор с размером турнира 2, т. е. получаем лучшего индивидуума среди случайно выбранных индивидуумов с размером турнира.\n",
    "* Имя mate зарегистрировано как псевдоним существующей в модуле tools функции _cxSimulatedBinaryBounded_ с аргументами low=BOUNDS_LOW, up=BOUNDS_HIGH и eta=CROWDING_FACTOR, т.е. _cxSimulatedBinaryBounded_ выполняет имитацию двоичного кроссовера, которая изменяет входные элементы на месте. Имитируемый двоичный кроссовер ожидает, что последовательность элементов чисел с плавающей запятой. В данном случае принимает аргументы low и up – нижнюю и верхнюю границы области поиска соответственно, а аргумент eta – степень скученности кроссовера. При высоком eta дети будут похожи на своих родителей, в то время как при небольшом eta решения будут гораздо более отличаться.\n",
    "* Имя mate зарегистрировано как псевдоним существующей в модуле tools функции _mutPolynomialBounded_. _mutPolynomialBounded_ – полиномиальная мутация, реализованная в оригинальном алгоритме NSGA-II на C Deb. С аргументами low и up, eta тоже самое, а indpb – это независимая вероятность мутации каждого атрибута."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем начальную популяцию оператором populationCreator, задавая размер популяции _POPULATION_SIZE_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать начальную популяцию (поколение 0):\n",
    "population = toolbox.populationCreator(n=POPULATION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сбора статистики мы воспользуемся классом _tools.Statistics_, предоставляемым _DEAP_. Он позволяет собирать статистику, задавая функцию, применяемую к данным, для которых вычисляется статистика.\n",
    "\n",
    "Поскольку в нашем случае данными является популяция, зададим функцию, которая извлекает приспособленность каждого индивидуума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовьте объект статистики:\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зарегистрируем функции, применяемые к этим значениям на каждом шаге. В нашем примере это функции _NumPy max, min, mean_ и _std_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.register(\"max\", numpy.max)\n",
    "stats.register(\"min\", numpy.min)\n",
    "stats.register(\"avg\", numpy.mean)\n",
    "stats.register(\"std\", numpy.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "собранная статистика возвращается в объекте logbook в конце работы программы\n",
    "\n",
    "У метода _eaSimpleWithElitism_ есть еще одна возможность – зал славы (hall of fame, сокращенно hof). Класс HallOfFame, находящийся в модуле tools, позволяет сохранить лучших индивидуумов, встретившихся в процессе эволюции, даже если вследствие отбора, скрещивания и мутации они были в какой-то момент утрачены. Зал славы поддерживается в отсортированном состоянии, так что первым элементом всегда является индивидуум с наилучшим встретившимся значением приспособленности.\n",
    "\n",
    "По завершении алгоритма атрибут items объекта HallOfFame можно использовать для доступа к списку помещенных в зал славы индивидуумов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определите объект зала славы:\n",
    "hof = tools.HallOfFame(HALL_OF_FAME_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем элитистский подход, т. е. без изменения копировать лучших на данный момент индивидуумов из зала славы в следующее поколение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmax   \tmin\tavg     \tstd      \n",
      "0  \t20    \t0.8022\t0.5\t0.605098\t0.0847468\n",
      "1  \t14    \t0.8022\t0.5\t0.68401 \t0.0752981\n",
      "2  \t11    \t0.81408\t0.5\t0.728172\t0.0798306\n",
      "3  \t14    \t0.81408\t0.5\t0.769478\t0.0729567\n",
      "4  \t11    \t0.81408\t0.78792\t0.804044\t0.00520403\n",
      "5  \t14    \t0.81408\t0.7958 \t0.80563 \t0.00441642\n",
      "6  \t7     \t0.81408\t0.7956 \t0.805772\t0.00520066\n",
      "7  \t15    \t0.81636\t0.79088\t0.806046\t0.0071177 \n",
      "8  \t12    \t0.81644\t0.7518 \t0.804574\t0.013593  \n",
      "9  \t14    \t0.81644\t0.79776\t0.807442\t0.00577423\n",
      "10 \t14    \t0.8174 \t0.79236\t0.806532\t0.00819452\n"
     ]
    }
   ],
   "source": [
    "# выполнение генетического алгоритма с добавленной функцией hof:\n",
    "population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER,mutpb=P_MUTATION,ngen=MAX_GENERATIONS,stats=stats,halloffame=hof,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После прогона алгоритма для 10 поколений с размером популяции 20 получается результат выше. Она автоматически генерируется методом _eaSimpleWithElitism_ в соответствии с переданным ему объектом статистики, если аргумент verbose равен True.\n",
    "\n",
    "Печатаем лучшее решение и получем графики изменеия нашей статистики в течении 10 поколений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наилучшее решение:\n",
    "print(\"- Best solution is: \")\n",
    "print(hof.items[0])\n",
    "print(\"params = \", test.formatParams(hof.items[0]))\n",
    "#print(\"params = \", \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'max_leaf_nodes'=%r, 'bootstrap'=%r, 'class_weight'=%s\" % test.convertParams(hof.items[0]))\n",
    "print(\"Accuracy = %.5f\" % hof.items[0].fitness.values[0])\n",
    "\n",
    "# Извлекаем статистику:\n",
    "maxFitnessValues, minFitnessValues, meanFitnessValues, stdFitnessValues = logbook.select(\"max\", \"min\", \"avg\", \"std\")\n",
    "\n",
    "# Построения графиков статистики:\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(maxFitnessValues, color='red', label = 'Max accuracy in generation')\n",
    "plt.plot(minFitnessValues, color='blue', label = 'Min accuracy in generation')\n",
    "plt.plot(meanFitnessValues, color='green', label = 'Average accuracy in generation')\n",
    "#plt.plot(stdFitnessValues, color='black', label = 'Standard deviation accuracy in generation')\n",
    "plt.legend() \n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Max / Min / Average / Standard deviation Fitness')\n",
    "plt.title('Max, Min, Average and Standard deviation fitness over Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"4.1.1.png\"></center>\n",
    "<center>Рис. 1. Статистика в течении 10 поколений</center>\n",
    "\n",
    "После прогона алгоритма для 10 поколений с размером популяции 20 получаем лучший результат с точностью равной 0.81740. С гиперпараметрами классификатора RandomForestClassifier:\n",
    "_n_estimators = 131, criterion = entropy, max_depth = 4, min_samples_split = 3, min_samples_leaf = 5, max_features = sqrt, max_leaf_nodes = 5, bootstrap = True, class_weight = balanced_subsample, ccp_alpha = 0.00020_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 2. Построение многоклассого классификатора.\n",
    "\n",
    "**Целью этапа:** является создание многоклассового классификатора новостных лент из набора данных Reuters, использовав для улучшения качества модели генетические алгоритмы. \n",
    "\n",
    "**Формулировка задания:** создать сеть для классификации новостных лент агентства Reuters на 46 взаимоисключающих тем, с помощью генетического алгоритма улучшить качество модели машинного обучения с учителем RandomForestClassifier за счёт настройки его гиперпараметров и классифицировать новостные ленты.\n",
    "\n",
    "Загрузка набора данных Reuters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сравнению с предыдущем этапом лабораторной работы классов больше двух, значит эта задача относится к категории задач многоклассовой классификации; и, поскольку каждый экземпляр данных должен быть отнесен только к одному классу, эта задача является примером однозначной многоклассовой классификации. \n",
    "\n",
    "Reuters - простой набор данных, широко используемых для классификации текста. Существует 46 разных тем; некоторые темы более широко представлены, некоторые менее, но для каждой из них в обучающем наборе имеется не менее 10 примеров.\n",
    "\n",
    "Было решено убрать некоторые параметры модели _RandomForestClassifier_, так как из-за их использования точность прогнозирования модели падала до 0.0, соответсвенно были убраны следующие гиперпараметры:\n",
    "1. max_leaf_nodes,\n",
    "2. class_weight,\n",
    "3. ccp_alpha.\n",
    "\n",
    "Также для улучшения точности и скорости были изменеы нижние и верхние границы допустимых значений слудеющих гиперпараметров:\n",
    "1. n_estimators: от 100 до 200,\n",
    "3. max_depth: от 2 до 20,\n",
    "4. min_samples_split: от 2 до 20,\n",
    "5. min_samples_leaf: от 2 до 20.\n",
    "\n",
    "Количество поколений уменьшим до 10, и популяция в каждом поколении тоже уменьшим до 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: int, \n",
    "# criterion: string, \n",
    "# max_depth: int_None, \n",
    "# min_samples_split: int,\n",
    "# min_samples_leaf: int,\n",
    "# max_features: string_None, \n",
    "# bootstrap: bool.\n",
    "\n",
    "BOUNDS_LOW =  [50, 0, 0, 2, 2, 0, 2, 0, 0, 0.0]\n",
    "BOUNDS_HIGH = [200, 2, 20, 20, 20, 2, 20, 1, 2, 0.05]\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Константы генетического алгоритма:\n",
    "POPULATION_SIZE = 20 \n",
    "P_CROSSOVER = 0.9  # вероятность пересечения\n",
    "P_MUTATION = 0.2 # вероятность мутации индивидуума\n",
    "MAX_GENERATIONS = 10\n",
    "HALL_OF_FAME_SIZE = 5\n",
    "CROWDING_FACTOR = 20.0  # фактор вытеснения для скрещивания и мутации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим класс для получениия оценки верности классификатора обученной на данных из Reuters: \n",
    "\n",
    "Напиишем класс _HyperparameterTuningGenetic_ для оценки верности классификатора:\n",
    "\n",
    "По сравнению с предыдущим этапом были изменены следющие функции: \n",
    "Метод класса _initIMDBDataset_ изменился на _initReutersDataset_, он выгружает данные из набора данных _reuters_ и вызывает метод _Vectorize_sequences_ для векторизации тренировочных и сестовых данных, также векторизует метки с помощью прямого кодирования (one-hot encoding). Прямое кодирование широко используется для форматирования категорий и также называется кодированием категорий (categorical encoding). Аргумент _num_words=10000_ означает, что в обучающих данных будет сохранено только 10000 слов, наиболее часто встречающихся в обучающем наборе отзывов, остальные слова будут отброшены.\n",
    "\n",
    "Переменные _train_data_ и _test_data_ — это списки новостных лент, каждая новость — это список индексов слов (кодированное представление последовательности слов).\n",
    "\n",
    "Переменные _train_labels_ и _test_labels_ — это списки меток определяющий класс принадлежности новостных лент от 0 до 45.\n",
    "\n",
    "Метод класса _Vectorize_sequences_ остался неизменным.\n",
    "\n",
    "Метод класса _convertParam_ как и _formatParams_ изменились только в количестве обработки гиперпараметров.\n",
    "\n",
    "Метод класса _getAccuracy_ осталя неизменным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuningGenetic:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initReutersDataset()\n",
    "        # self.kfold = model_selection.KFold(n_splits=self.NUM_FOLDS, random_state=self.randomSeed)\n",
    "        \n",
    "    def initReutersDataset(self):\n",
    "        (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "        self.x_train = self.Vectorize_sequences(train_data)\n",
    "        self.x_test = self.Vectorize_sequences(test_data)\n",
    "        self.y_train = to_categorical(train_labels)\n",
    "        self.y_test = to_categorical(test_labels)\n",
    "    \n",
    "    def Vectorize_sequences(self, sequences, dimension=10000):\n",
    "        results = numpy.zeros((len(sequences), dimension))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            results[i, sequence] = 1.\n",
    "        return results\n",
    "    \n",
    "    def convertParams(self, params):\n",
    "        n_estimators = round(params[0])\n",
    "        criterion = ['gini', 'entropy', 'log_loss'][round(params[1])]\n",
    "        max_depth = None if round(params[2]) == 0 else round(params[2])\n",
    "        min_samples_split = round(params[3])\n",
    "        min_samples_leaf = round(params[4])\n",
    "        max_features = ['sqrt', 'log2', None][round(params[5])]\n",
    "        bootstrap = True if round(params[6]) == 1 else False\n",
    "        return n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap\n",
    "\n",
    "    def getAccuracy(self, params):\n",
    "        n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap = self.convertParams(params)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features, bootstrap=bootstrap)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        return accuracy_score(self.y_test, y_pred)\n",
    "    \n",
    "    def formatParams(self, params):\n",
    "        return \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'bootstrap'=%r\" % (self.convertParams(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод _eaSimpleWithElitism_() был взят из первого этапа лабораторной работы без изменения.\n",
    "Как и последующий код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__): # элитарность\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    # Этот алгоритм аналогичен алгоритму Deeper Simple() с той модификацией, \n",
    "    # что halloffame используется для реализации механизма элитарности. \n",
    "    # Особи, содержащиеся в halloffame, напрямую передаются следующему поколению и \n",
    "    # не подвергаются генетическим операторам отбора, скрещивания и мутации\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Оценивайте \"individuals\" с недостаточной \"fitness\"\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\") \n",
    "        # параметр halloffame не должен быть пустым!\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Начните процесс смены поколений\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Выберите людей следующего поколения\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Расширяйте круг \"individuals\"\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Оценивайте \"individuals\" с недостаточной \"fitness\"\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # добавляйте лучших обратно в популяцию:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Обновите зал славы сгенерированными \"individuals\"\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Замените текущую популяцию потомством\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Добавьте статистику текущей генерации в журнал регистрации\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = HyperparameterTuningGenetic()\n",
    "toolbox = base.Toolbox()\n",
    "# определите единую цель, максимизирующую фитнес-стратегию:\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "# создайте индивидуальный класс на основе списка:\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "# определите атрибуты гиперпараметра индивидуально:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"hyperparameter_0\", \"hyperparameter_1\", ...\n",
    "    toolbox.register(\"hyperparameter_\" + str(i),# расширить\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "# создайте кортеж, содержащий генератор атрибутов для каждого искомого параметра:\n",
    "hyperparameters = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    hyperparameters = hyperparameters + \\\n",
    "                      (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)\n",
    "# создайте отдельный оператор для заполнения Individual экземпляра:\n",
    "toolbox.register(\"individualCreator\",# исправить\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual,\n",
    "                 hyperparameters,\n",
    "                 n=1)\n",
    "# создайте оператора population для создания списка individual:\n",
    "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.individualCreator)\n",
    "# fitness calculation\n",
    "def classificationAccuracy(individual):\n",
    "    return test.getAccuracy(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", classificationAccuracy)\n",
    "\n",
    "# генетические операторы:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2) # турнирный отбор = 2\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,# ограниченный вариант оператора cxSimulatedBinary(), принимающий аргументы low и up – нижнюю и верхнюю границы области поиска соответственно\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0 / NUM_OF_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать начальную популяцию (поколение 0):\n",
    "population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "# подготовьте объект статистики:\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"max\", numpy.max)\n",
    "stats.register(\"min\", numpy.min)\n",
    "stats.register(\"avg\", numpy.mean)\n",
    "stats.register(\"std\", numpy.std)\n",
    "# определите объект зала славы:\n",
    "hof = tools.HallOfFame(HALL_OF_FAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполните выполнение генетического алгоритма с добавленной функцией hof:\n",
    "population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER,mutpb=P_MUTATION,ngen=MAX_GENERATIONS,stats=stats,halloffame=hof,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наилучшее решение:\n",
    "print(\"- Best solution is: \")\n",
    "print(hof.items[0])\n",
    "print(\"params = \", test.formatParams(hof.items[0]))\n",
    "#print(\"params = \", \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'max_leaf_nodes'=%r, 'bootstrap'=%r, 'class_weight'=%s\" % test.convertParams(hof.items[0]))\n",
    "print(\"Accuracy = %.5f\" % hof.items[0].fitness.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем статистику:\n",
    "maxFitnessValues, minFitnessValues, meanFitnessValues, stdFitnessValues = logbook.select(\"max\", \"min\", \"avg\", \"std\")\n",
    "\n",
    "# Построения графиков статистики:\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(maxFitnessValues, color='red', label = 'Max accuracy in generation')\n",
    "#plt.plot(minFitnessValues, color='blue', label = 'Min accuracy in generation')\n",
    "#plt.plot(meanFitnessValues, color='green', label = 'Average accuracy in generation')\n",
    "#plt.plot(stdFitnessValues, color='black', label = 'Standard deviation accuracy in generation')\n",
    "plt.legend() \n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Max Fitness')\n",
    "plt.title('Max fitness over Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"4.2.11.png\"></center>\n",
    "<center>Рис. 2. Статистика в течении 10 поколений</center>\n",
    "После прогона алгоритма для 10 поколений с размером популяции 10 получаем лучши результат с точностью равной 0.66696. С гиперпараметрами классификатора RandomForestClassifier:\n",
    "_n_estimators = 93, criterion = log_loss, max_depth = None, min_samples_split = 5, min_samples_leaf = 2, max_features = None, bootstrap = False._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 3. Построение прогноза на основе регрессионной модели\n",
    "**Целью этапа:** является создание модели машинного обучения _RandomForestRegressor_, дающую прогноз цен на дома из набора данных Boston Housing, использовав для улучшения качества модели генетические алгоритмы.\n",
    "\n",
    "**Формулировка задания:** построить регрессионную модель для предсказания медианной цены на дома в пригороде Бостона, с помощью генетического алгоритма улучшить качество модели машинного обучения с учителем _RandomForestRegressor_ за счёт настройки его гиперпараметров и предсказать цены на дома в пригороде Бостона.\n",
    "\n",
    "Загрузка набора данных Boston Housing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом этапе мы попытаемся предсказать медианную цену на дома в пригороде Бостона в середине 1970 года. Для этого используем регрессионную модель _RandomForestRegressor_.\n",
    "Основное отличие скалярной регрессии от двух предыдущих этапов, заключается в предсказании не дискретной метки, а значения на непрерывной числовой прямой: например, температуры воздуха на завтра по имеющимся метеорологическим данным или предсказание времени завершения программного проекта по его спецификациям.\n",
    "\n",
    "В модели _RandomForestRegressor_ существует множество гиперпараметров, они почти индетичны по сравнению с моделью _RandomForestClassifier_ в этом этапе мы используем следющие гиперпараметры:\n",
    "1. n_estimators – количество деревьев в лесу.\n",
    "2. criterion - Функция для измерения качества разделения. Поддерживаемые критерии: “squared_error” для среднеквадратичной ошибки, которая равна уменьшению дисперсии в качестве критерия выбора объекта и минимизирует потерю L2, используя среднее значение каждого терминального узла, “friedman_mse”, который использует среднеквадратичную ошибку с оценкой улучшения Фридмана для потенциальных разделений, “absolute_error” для средней абсолютной ошибки, которая минимизирует потерю L1, используя медиану каждого терминального узла, и “poisson”, который использует уменьшение Отклонение Пуассона для поиска расколов. Обучение с использованием “absolute_error” происходит значительно медленнее, чем при использовании “squared_error\".\n",
    "3. max_depth - максимальная глубина дерева. Если None, то узлы расширяются до тех пор, пока все листья не станут чистыми или пока все листья не будут содержать меньше выборок min_samples_split.\n",
    "4. min_samples_split - минимальное число выборок, для разделения внутреннего узла.\n",
    "5. min_samples_leaf - минимальное число образцов должны быть на листе. Точка разделения на любой глубине будет рассматриваться только в том случае, если она оставляет не менее min_samples_leaf образцов подготовки в каждой из левой и правой ветвей. Это может привести к сглаживанию модели, особенно при регрессии.\n",
    "6. max_features - количество функций, которые следует учитывать при поиске наилучшего разделения: если “sqrt”, то max_features=sqrt(n_features), если “log2”, то max_features=log2(n_features), если None, то max_features=n_features, где n_features это количество особенностей, наблюдаемых во время подгонки (fit).\n",
    "7. max_leaf_nodes - выращивайте деревья с помощью max_leaf_nodes по принципу \"сначала лучше\". Лучшие узлы определяются как относительное уменьшение примесей. Если None, то неограниченное количество конечных узлов.\n",
    "8. bootstrap - используются ли выборки bootstrap при построении деревьев. Если значение равно False, для построения каждого дерева используется весь набор данных.\n",
    "9. ccp_alpha - параметр сложности, используемый для сокращения сложности с минимальными затратами. Будет выбрано поддерево с наибольшей сложностью затрат, которое меньше ccp_alpha. По умолчанию обрезка не выполняется.\n",
    "\n",
    "В процессе тестирования программы было замечено, что модель работает быстрее чем , поэтому было решено изменить границы следующих гиперпараметров:\n",
    "\n",
    "1. n_estimators: от 100 до 1000,\n",
    "3. max_depth: от 0 до 100,\n",
    "4. min_samples_split: от 2 до 20,\n",
    "5. min_samples_leaf: от 2 до 20.\n",
    "6. max_leaf_nodes: от 2 до 20.\n",
    "\n",
    "Также повысим значение популяции в каждом поколении до 100 и количество поколений до 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: int, \n",
    "# criterion: string, \n",
    "# max_depth: int_None, \n",
    "# min_samples_split: int,\n",
    "# min_samples_leaf: int,\n",
    "# max_features: string_None, \n",
    "# max_leaf_nodes: int_None,\n",
    "# bootstrap: bool, \n",
    "# ccp_alpha: non-negative float\n",
    "\n",
    "BOUNDS_LOW =  [100, 0, 0, 2, 2, 0, 2, 0, 0.0]\n",
    "BOUNDS_HIGH = [1000, 3, 100, 20, 20, 2, 20, 1, 0.05]\n",
    "\n",
    "NUM_OF_PARAMS = len(BOUNDS_HIGH)\n",
    "\n",
    "# Константы генетического алгоритма:\n",
    "POPULATION_SIZE = 100 \n",
    "P_CROSSOVER = 0.9  # вероятность пересечения\n",
    "P_MUTATION = 0.2   # вероятность мутации индивидуума\n",
    "MAX_GENERATIONS = 20\n",
    "HALL_OF_FAME_SIZE = 5\n",
    "CROWDING_FACTOR = 20.0  # фактор вытеснения для скрещивания и мутации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим класс для получениия оценки верности регресионной модели обученной на данных из _boston_housing_: \n",
    "\n",
    "Напиишем класс _HyperparameterTuningGenetic_ для оценки верности регресионной модели:\n",
    "\n",
    "По сравнению с предыдущим этапом были изменены следющие функции: \n",
    "Метод класса _initReutersDataset_ изменился на _initBostonDataset_, он выгружает данные из набора данных _boston_housing_ и нормализует данные так как данные имеют самые разные диапазоны. Суть нормализации состоит в том что для каждого признака во входных данных (столбца в матрице входных данных) из каждого значения вычитается среднее по этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение, равное единице.\n",
    "\n",
    "\n",
    "Для предсказания воспользуемся данными, как уровень преступности, ставка местного имущественного налога и т. д. Используемый набор данных, имеет интересное отличие от двух предыдущих этапов. Он содержит относительно немного образцов данных: всего 506, разбитых на 404 обучающих и 102 контрольных образца. И каждый признак во входных данных (например, уровень преступности) имеет свой масштаб. Например, некоторые признаки являются пропорциями и имеют значения между 0 и 1, другие - между 1 и 12 и т.д.\n",
    "\n",
    "Метод класса _Vectorize_sequences_ был удалён.\n",
    "\n",
    "Метод класса _convertParam_ как и _formatParams_ изменились только в количестве обработки гиперпараметров.\n",
    "\n",
    "Метод класса _getAccuracy_ поменял модель на _RandomForestRegressor_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuningGenetic:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initBostonDataset()\n",
    "        \n",
    "    def initBostonDataset(self):\n",
    "        (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "        mean=train_data.mean(axis=0)\n",
    "        train_data-=mean\n",
    "        std=train_data.std(axis=0)\n",
    "        train_data/=std\n",
    "        test_data-=mean\n",
    "        test_data/=std\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.train_targets = train_targets\n",
    "        self.test_data = test_data\n",
    "        self.test_targets = test_targets\n",
    "    \n",
    "    def convertParams(self, params):\n",
    "        n_estimators = round(params[0])\n",
    "        criterion = ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'][round(params[1])]\n",
    "        max_depth = None if round(params[2]) == 0 else round(params[2])\n",
    "        min_samples_split = round(params[3])\n",
    "        min_samples_leaf = round(params[4])\n",
    "        max_features = ['sqrt', 'log2', None][round(params[5])]\n",
    "        max_leaf_nodes = None if round(params[6]) == 0 else round(params[6])\n",
    "        bootstrap = True if round(params[7]) == 1 else False\n",
    "        ccp_alpha = params[8]\n",
    "        return n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, ccp_alpha\n",
    "\n",
    "    def getAccuracy(self, params):\n",
    "        n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, ccp_alpha = self.convertParams(params)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features, max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, ccp_alpha=ccp_alpha)\n",
    "        \n",
    "        model.fit(self.train_data, self.train_targets)\n",
    "        prediction = model.predict(self.test_data)\n",
    "        return mean_absolute_error(self.test_targets, prediction)\n",
    "    \n",
    "    def formatParams(self, params):\n",
    "        return \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'max_leaf_nodes'=%r, 'bootstrap'=%r, 'ccp_alpha'=%.5f\" % (self.convertParams(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод _eaSimpleWithElitism_() был взят из первого этапа лабораторной работы без изменения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__): # элитарность\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    # Этот алгоритм аналогичен алгоритму Deeper Simple() с той модификацией, \n",
    "    # что halloffame используется для реализации механизма элитарности. \n",
    "    # Особи, содержащиеся в halloffame, напрямую передаются следующему поколению и \n",
    "    # не подвергаются генетическим операторам отбора, скрещивания и мутации\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Оценивайте \"individuals\" с \"fitness\"\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\") \n",
    "        # параметр halloffame не должен быть пустым!\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Начните процесс смены поколений\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Выберите людей следующего поколения\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Расширяйте круг \"individuals\"\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Оценивайте \"individuals\" с \"fitness\"\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # добавляйте лучших обратно в популяцию:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Обновите зал славы сгенерированными \"individuals\"\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Замените текущую популяцию потомством\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Добавьте статистику текущей генерации в журнал регистрации\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как наша цель минимизировать оценку mae (mean absolute error).\n",
    "\n",
    "mae - средняя абсолютная ошибка. Это абсолютное значение разности предсказанными и целевыми значениями.\n",
    "\n",
    "Мы должны изменить стратегию: \n",
    "\n",
    "Поскольку мы стремимся минимизировать ошибку регрессионной модели, определим единственную цель – минмизирующую стратегию приспособления _FitnessMin_, а веса _weights_ станут отрицательными значениями. \n",
    "\n",
    "Следовательно функция приспособленности класса _Individual_ в качестве атрибута станет _FitnessMin_.\n",
    "\n",
    "Остальное останется без изменения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = HyperparameterTuningGenetic()\n",
    "toolbox = base.Toolbox()\n",
    "# определите единую цель, максимизирующую фитнес-стратегию:\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "# создайте индивидуальный класс на основе списка:\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "# определите атрибуты гиперпараметра индивидуально:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"hyperparameter_0\", \"hyperparameter_1\", ...\n",
    "    toolbox.register(\"hyperparameter_\" + str(i),# расширить\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "# создайте кортеж, содержащий генератор атрибутов для каждого искомого параметра:\n",
    "hyperparameters = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    hyperparameters = hyperparameters + \\\n",
    "                      (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)\n",
    "# создайте отдельный оператор для заполнения Individual экземпляра:\n",
    "toolbox.register(\"individualCreator\",# исправить\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual,\n",
    "                 hyperparameters,\n",
    "                 n=1)\n",
    "# создайте оператора population для создания списка individual:\n",
    "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.individualCreator)\n",
    "# fitness calculation\n",
    "def classificationAccuracy(individual):\n",
    "    return test.getAccuracy(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", classificationAccuracy)\n",
    "# генетические операторы:mutFlipBit\n",
    "\n",
    "# генетические операторы:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2) # турнирный отбор = 2\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,# ограниченный вариант оператора cxSimulatedBinary(), принимающий аргументы low и up – нижнюю и верхнюю границы области поиска соответственно\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)# коэффициент скученности (eta), задействованный в операциях скрещивания и мутации:\n",
    "# cxSimulatedBinary() – реализация имитации двоичного скрещивания, величина η передается в параметре eta;\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0 / NUM_OF_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать начальную популяцию (поколение 0):\n",
    "population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "# подготовьте объект статистики:\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values) # разобраться\n",
    "stats.register(\"min\", numpy.min)\n",
    "stats.register(\"max\", numpy.max)\n",
    "stats.register(\"avg\", numpy.mean)\n",
    "stats.register(\"std\", numpy.std)\n",
    "# определите объект зала славы:\n",
    "hof = tools.HallOfFame(HALL_OF_FAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmin    \tmax    \tavg    \tstd     \n",
      "0  \t100   \t2.64809\t5.93212\t3.30172\t0.564761\n",
      "1  \t85    \t2.61518\t4.60677\t3.00661\t0.257467\n",
      "2  \t89    \t2.61518\t3.67322\t2.92455\t0.187918\n",
      "3  \t87    \t2.58659\t3.28176\t2.83231\t0.166791\n",
      "4  \t91    \t2.58659\t3.31195\t2.74736\t0.148758\n",
      "5  \t85    \t2.58659\t3.23223\t2.68948\t0.119033\n",
      "6  \t87    \t2.57681\t3.0819 \t2.637  \t0.0660754\n",
      "7  \t90    \t2.55632\t2.80247\t2.6112 \t0.0327832\n",
      "8  \t86    \t2.55632\t2.66249\t2.60587\t0.0206812\n",
      "9  \t92    \t2.55632\t2.84633\t2.60766\t0.0458618\n",
      "10 \t88    \t2.55632\t2.81063\t2.59713\t0.0313357\n",
      "11 \t87    \t2.55632\t2.91076\t2.59581\t0.0441575\n",
      "12 \t87    \t2.55462\t2.84654\t2.59217\t0.0334119\n",
      "13 \t86    \t2.54478\t2.80444\t2.5876 \t0.0269018\n",
      "14 \t91    \t2.54478\t3.07766\t2.59947\t0.0765501\n",
      "15 \t92    \t2.54478\t3.1138 \t2.59431\t0.0563192\n",
      "16 \t89    \t2.54478\t2.81143\t2.59327\t0.0347732\n",
      "17 \t91    \t2.53743\t2.66135\t2.587  \t0.0207428\n",
      "18 \t86    \t2.53743\t3.07766\t2.59135\t0.0624206\n",
      "19 \t91    \t2.53743\t3.1138 \t2.59257\t0.0623328\n",
      "20 \t84    \t2.53743\t2.94387\t2.58966\t0.0406859\n"
     ]
    }
   ],
   "source": [
    "# выполните выполнение генетического алгоритма с добавленной функцией hof:\n",
    "population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER,mutpb=P_MUTATION,ngen=MAX_GENERATIONS,stats=stats,halloffame=hof,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Best solution is: \n",
      "[208.92015504714593, 0.1616585288592741, 61.079663038591164, 4.002559822044243, 6.955693971347826, 1.5264439394520408, 19.973491323005792, 0.7549652326332608, 0.01909447049869757]\n",
      "params =  'n_estimators'=209, 'criterion'=squared_error, 'max_depth'=61, 'min_samples_split'=4, 'min_samples_leaf'=7, 'max_features'=None, 'max_leaf_nodes'=20, 'bootstrap'=True, 'ccp_alpha'=0.01909\n",
      "MAE = 2.5374286287\n"
     ]
    }
   ],
   "source": [
    "# найдено наилучшее решение для печати:\n",
    "print(\"- Best solution is: \")\n",
    "print(hof.items[0])\n",
    "print(\"params = \", test.formatParams(hof.items[0]))\n",
    "print(\"MAE = %.10f\" % hof.items[0].fitness.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем статистику:\n",
    "minFitnessValues, maxFitnessValues, meanFitnessValues, stdFitnessValues = logbook.select(\"min\", \"max\", \"avg\", \"std\")\n",
    "\n",
    "# Построения графиков статистики:\n",
    "sns.set_style(\"whitegrid\")\n",
    "#plt.plot(maxFitnessValues, color='red', label = 'Max MAE in generation')\n",
    "plt.plot(minFitnessValues, color='blue', label = 'Min MAE in generation')\n",
    "#plt.plot(meanFitnessValues, color='green', label = 'Average MAE in generation')\n",
    "#plt.plot(stdFitnessValues, color='black', label = 'Standard deviation MAE in generation')\n",
    "plt.legend() \n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Min Fitness')\n",
    "plt.title('Min fitness over Generations')\n",
    "#plt.ylabel('Max / Min / Average / Standard deviation Fitness')\n",
    "#plt.title('Max, Min, Average and Standard deviation fitness over Generations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"4.3.11.png\"></center>\n",
    "<center>Рис. 3. Статистика в течении 20 поколений</center>\n",
    "\n",
    "После прогона алгоритма для 20 поколений с размером популяции 100 получаем лучший результат с ошибкой MAE равной 2.5374286287. С гиперпараметрами модели RandomForestRegressor: _n_estimators = 209, criterion = squared_error, max_depth = 61, min_samples_split = 4, min_samples_leaf = 7, max_features = None, max_leaf_nodes = 20, bootstrap = True, ccp_alpha = 0.01909_.\n",
    "\n",
    "Попробуем поменять способ обучения регрессионной модели, используя метод перкрестной проверки по K блокам. Подробнее о реализации написано в первой лабораторной работе.\n",
    "\n",
    "\n",
    "Класс _HyperparameterTuningGenetic_ для оценки верности регресионной модели изменится только в функции _getAccuracy_:\n",
    "Используем перекрестную проверку с помощью функции _cross_val_score_ и задаём метрику потери: neg_mean_absolute_error . neg_mean_absolute_error - это потеря регрессии с абсолютной ошибкой среднего значения с отрицательным знаком. Как ошибку получаем средее значение трех оценко mae, так как мы разбили выборку на 3 блока.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuningGenetic:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initBostonDataset()\n",
    "        \n",
    "    def initBostonDataset(self):\n",
    "        (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data(test_split=0)\n",
    "        self.X_data = train_data\n",
    "        self.y_data = train_targets\n",
    "        mean=self.X_data.mean(axis=0)\n",
    "        self.X_data-=mean\n",
    "        std=self.X_data.std(axis=0)\n",
    "        self.X_data/=std\n",
    "    \n",
    "    def convertParams(self, params):\n",
    "        n_estimators = round(params[0])\n",
    "        criterion = ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'][round(params[1])]\n",
    "        max_depth = None if round(params[2]) == 0 else round(params[2])\n",
    "        min_samples_split = round(params[3])\n",
    "        min_samples_leaf = round(params[4])\n",
    "        max_features = ['sqrt', 'log2', None][round(params[5])]\n",
    "        max_leaf_nodes = None if round(params[6]) == 0 else round(params[6])\n",
    "        bootstrap = True if round(params[7]) == 1 else False\n",
    "        ccp_alpha = params[8]\n",
    "        return n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, ccp_alpha\n",
    "\n",
    "    def getAccuracy(self, params):\n",
    "        n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, bootstrap, ccp_alpha = self.convertParams(params)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split = min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features, max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, ccp_alpha=ccp_alpha)\n",
    "        scores = cross_val_score(model, self.X_data, self.y_data, cv=3, scoring = 'neg_mean_absolute_error')\n",
    "        return scores.mean()\n",
    "    \n",
    "    def formatParams(self, params):\n",
    "        return \"'n_estimators'=%d, 'criterion'=%s, 'max_depth'=%r, 'min_samples_split'=%d, 'min_samples_leaf'=%d, 'max_features'=%s, 'max_leaf_nodes'=%r, 'bootstrap'=%r, 'ccp_alpha'=%.5f\" % (self.convertParams(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как наша оценка теперь стала отрицательной, то необходимо теперь максимизировать оценку mae.\n",
    "\n",
    "Мы должны исправить стратегию, и вернуть к виду, которые были использованы в 1 и 2 этапах. Остальное останется без изменения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = HyperparameterTuningGenetic()\n",
    "toolbox = base.Toolbox()\n",
    "# определите единую цель, максимизирующую фитнес-стратегию:\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "# создайте индивидуальный класс на основе списка:\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "# определите атрибуты гиперпараметра индивидуально:\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    # \"hyperparameter_0\", \"hyperparameter_1\", ...\n",
    "    toolbox.register(\"hyperparameter_\" + str(i),# расширить\n",
    "                     random.uniform,\n",
    "                     BOUNDS_LOW[i],\n",
    "                     BOUNDS_HIGH[i])\n",
    "# создайте кортеж, содержащий генератор атрибутов для каждого искомого параметра:\n",
    "hyperparameters = ()\n",
    "for i in range(NUM_OF_PARAMS):\n",
    "    hyperparameters = hyperparameters + \\\n",
    "                      (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)\n",
    "# создайте отдельный оператор для заполнения Individual экземпляра:\n",
    "toolbox.register(\"individualCreator\",# исправить\n",
    "                 tools.initCycle,\n",
    "                 creator.Individual,\n",
    "                 hyperparameters,\n",
    "                 n=1)\n",
    "# создайте оператора population для создания списка individual:\n",
    "toolbox.register(\"populationCreator\", tools.initRepeat, list, toolbox.individualCreator)\n",
    "# fitness calculation\n",
    "def classificationAccuracy(individual):\n",
    "    return test.getAccuracy(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", classificationAccuracy)\n",
    "# генетические операторы:mutFlipBit\n",
    "\n",
    "# генетические операторы:\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2) # турнирный отбор = 2\n",
    "toolbox.register(\"mate\",\n",
    "                 tools.cxSimulatedBinaryBounded,# ограниченный вариант оператора cxSimulatedBinary(), принимающий аргументы low и up – нижнюю и верхнюю границы области поиска соответственно\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR)# коэффициент скученности (eta), задействованный в операциях скрещивания и мутации:\n",
    "# cxSimulatedBinary() – реализация имитации двоичного скрещивания, величина η передается в параметре eta;\n",
    "toolbox.register(\"mutate\",\n",
    "                 tools.mutPolynomialBounded,\n",
    "                 low=BOUNDS_LOW,\n",
    "                 up=BOUNDS_HIGH,\n",
    "                 eta=CROWDING_FACTOR,\n",
    "                 indpb=1.0 / NUM_OF_PARAMS)\n",
    "# создать начальную популяцию (поколение 0):\n",
    "population = toolbox.populationCreator(n=POPULATION_SIZE)\n",
    "\n",
    "# подготовьте объект статистики:\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values) # разобраться\n",
    "stats.register(\"max\", numpy.max)\n",
    "stats.register(\"min\", numpy.min)\n",
    "stats.register(\"avg\", numpy.mean)\n",
    "stats.register(\"std\", numpy.std)\n",
    "# определите объект зала славы:\n",
    "hof = tools.HallOfFame(HALL_OF_FAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmax     \tmin     \tavg     \tstd     \n",
      "0  \t100   \t-2.56994\t-4.62699\t-3.09109\t0.313142\n",
      "1  \t90    \t-2.56184\t-3.44418\t-2.92395\t0.197716\n",
      "2  \t88    \t-2.51709\t-3.39014\t-2.81907\t0.178078\n",
      "3  \t87    \t-2.50589\t-3.0636 \t-2.75694\t0.156476\n",
      "4  \t84    \t-2.50173\t-3.1285 \t-2.70961\t0.16967 \n",
      "5  \t90    \t-2.47495\t-3.14807\t-2.62576\t0.140403\n",
      "6  \t84    \t-2.47495\t-3.17613\t-2.56338\t0.0932646\n",
      "7  \t75    \t-2.45004\t-2.72383\t-2.53161\t0.0452782\n",
      "8  \t86    \t-2.45004\t-2.71675\t-2.50746\t0.0356451\n",
      "9  \t93    \t-2.42566\t-2.54845\t-2.49104\t0.0238416\n",
      "10 \t85    \t-2.42566\t-2.66545\t-2.48051\t0.0303142\n",
      "11 \t84    \t-2.42361\t-2.51689\t-2.46524\t0.0194465\n",
      "12 \t82    \t-2.41388\t-2.65667\t-2.45822\t0.0278792\n",
      "13 \t90    \t-2.41388\t-2.52524\t-2.44708\t0.0147561\n",
      "14 \t91    \t-2.41388\t-2.63223\t-2.4419 \t0.029183 \n",
      "15 \t85    \t-2.41388\t-2.63825\t-2.43671\t0.0218377\n",
      "16 \t91    \t-2.41388\t-2.64046\t-2.43759\t0.02197  \n",
      "17 \t88    \t-2.41388\t-2.4573 \t-2.43285\t0.00734367\n",
      "18 \t85    \t-2.41388\t-2.65198\t-2.43618\t0.0235556 \n",
      "19 \t92    \t-2.41388\t-3.14494\t-2.44459\t0.0740114 \n",
      "20 \t89    \t-2.41388\t-2.4521 \t-2.43381\t0.00836072\n"
     ]
    }
   ],
   "source": [
    "# выполните выполнение генетического алгоритма с добавленной функцией hof:\n",
    "population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER,mutpb=P_MUTATION,ngen=MAX_GENERATIONS,stats=stats,halloffame=hof,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Best solution is: \n",
      "[499.60343835210404, 2.379211238128562, 23.322656699802153, 11.853219665696205, 2.088117760953659, 1.8322115001865766, 18.511106619897966, 0.7366816311420722, 0.001161024479828595]\n",
      "params =  'n_estimators'=500, 'criterion'=friedman_mse, 'max_depth'=23, 'min_samples_split'=12, 'min_samples_leaf'=2, 'max_features'=None, 'max_leaf_nodes'=19, 'bootstrap'=True, 'ccp_alpha'=0.00116\n",
      "MAE = -2.4138813583\n"
     ]
    }
   ],
   "source": [
    "# найдено наилучшее решение для печати:\n",
    "print(\"- Best solution is: \")\n",
    "print(hof.items[0])\n",
    "print(\"params = \", test.formatParams(hof.items[0]))\n",
    "print(\"MAE = %.10f\" % hof.items[0].fitness.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем статистику:\n",
    "maxFitnessValues, minFitnessValues, meanFitnessValues, stdFitnessValues = logbook.select(\"max\", \"min\", \"avg\", \"std\")\n",
    "\n",
    "# Построения графиков статистики:\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(maxFitnessValues, color='red', label = 'Max accuracy in generation')\n",
    "#plt.plot(minFitnessValues, color='blue', label = 'Min accuracy in generation')\n",
    "#plt.plot(meanFitnessValues, color='green', label = 'Average accuracy in generation')\n",
    "#plt.plot(stdFitnessValues, color='black', label = 'Standard deviation accuracy in generation')\n",
    "plt.legend() \n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Max Fitness')\n",
    "plt.title('Max fitness over Generations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"4.3.21.png\"></center>\n",
    "<center>Рис. 4. Статистика в течении 20 поколений</center>\n",
    "\n",
    "После прогона алгоритма для 20 поколений с размером популяции 100 получаем лучший результат с ошибкой MAE равной -2.4138813583. С гиперпараметрами модели RandomForestRegressor:\n",
    "_n_estimators = 500, criterion = friedman_mse, max_depth = 23, min_samples_split = 12, min_samples_leaf = 2, max_features = None, max_leaf_nodes = 19, bootstrap = True, ccp_alpha = 0.00116\n",
    "\n",
    "Средняя оценка ошибки прогнозирования составляет около 2372 долларов США.\n",
    "\n",
    "<p style=\"text-align: center;\">Заключение</p>\n",
    "\n",
    "В этой лабораторной работе мы занимались настройкой различных гиперпараметров Random Forest – алгоритма машинного обучения. Использовали каркас DEAP, которая поддерживает разработку решений с применением генетических алгоритмов и других методов эволюционных вычислений, и с помощью него написали генетический алгоритм для наших задач. Познакомились с его различными модулями: _creator_, _register_ и классами: _Fitness_, _Individual_, _Toolbox_. Использовали элитистский подход в функции _eaSimpleWithElitism_ для получения лучших решений, защитив их от применения операторов отбора, скрещивания и мутации.\n",
    "\n",
    "1. На первом этапе мы классифицировали отзывы к фильмам на положительные и отрицательные опираясь на текст отзывов. Использовали набор данных IMDB с 50000 отзывами. Подготовили данные для передачи в модель с помощью прямого кодирования списков, т.е. закодировали последовательности целых чисел в бинарную матрицу. Написали класс _HyperparameterTuningGenetic_ для оценки верности классификатора. В классе использовали следующие методы: Метод класса _initIMDBDataset_ выгружает данные из набора данных imdb и вызывает метод _Vectorize_sequences_ для векторизации тренировочных и тестовых данных, также векторизует метки. Метод класса _Vectorize_sequences_ принимает список отзывов и выполняет над ним прямое кодирование списков в векторы нулей и единиц и возвращает 10000-мерный вектор. Метод класса _convertParam_ принимает список params, содержащий значения гиперпараметров типа float, и возвращает преобразованные гиперпараметры в истинных значениях. Метод класса _getAccuracy_ принимает список чисел типа float, представляющих значения гиперпараметров, вызывает метод convertParam() для преобразования их в истинные значения и инициализирует классификатор _RandomForestClassifier_ с этими значениями. Затем он вычисляет точность классификатора. Метод класса _formatParams_ принимает список params, вызывает метод _convertParam_ для преобразования их в истинные значения, и возвращает строку, которая показывает значения гиперпараметров более информативно. После прогона алгоритма на 10 поколений с размером популяции 20 получаем лучший результат с точностью равной 81.17%. С гиперпараметрами классификатора _n_estimators = 131, criterion = entropy, max_depth = 4, min_samples_split = 3, min_samples_leaf = 5, max_features = sqrt, max_leaf_nodes = 5, bootstrap = True, class_weight = balanced_subsample, ccp_alpha = 0.00020_. Сравнивая с моделью, которая была построена на первом этапе первой лабораторной работе, это сеть состяла из двух слоев, с 64 нейронами, функции активации скрытых слоев tanh-relu, на выходном слое Dense была использована функция sigmoid, которая на выходе дает скалярное значение в диапозоне от 0 до 1, была использована бинарная перекрестная энтропия, как функция потерь. Скомпелировали и обучили модель сначала на 20 эпохах, но появляется эффект переобучения сети после третьей эпохи, потом переобучили сеть на трех эпохах, в итоге получили модель показывающаяся точность в 88.06% при потерях в 0.29. В итоге, классификатор _RandomForestClassifier_ проиграл в точности на 7%.\n",
    "\n",
    "\n",
    "2. На втором этапе мы классифицировали новостные ленты по их темам, опираясь на их текст. Использовали набор данных Reuters c 11228 новостей. Подготовили данные для передачи в сеть с помощью прямого кодирования как в предыдущем этапе лабораторной работы. Написали класс _HyperparameterTuningGenetic_ для оценки верности классификатора. По сравнению с предыдущим этапом были изменены следющие функции класса: метод класса _initIMDBDataset_ изменился на _initReutersDataset_, он выгружает данные из набора данных _reuters_ и вызывает метод _Vectorize_sequences_ для векторизации тренировочных и сестовых данных, также векторизует метки с помощью прямого кодирования (one-hot encoding). Метод класса _Vectorize_sequences_ остался неизменным. Метод класса _convertParam_ как и _formatParams_ изменились только в количестве обработки гиперпараметров. Метод класса _getAccuracy_ осталя неизменным. После прогона алгоритма на 10 поколений с размером популяции 20 получаем лучший результат с точностью равной 66.69%. С гиперпараметрами классификатора RandomForestClassifier: _n_estimators = 93, criterion = log_loss, max_depth = None, min_samples_split = 5, min_samples_leaf = 2, max_features = None, bootstrap = False._. Сравнивая с моделью, которая была построена на втором этапе первой лабораторной работе, это сеть состяла из двух слоев, на первом слое было использовано 4 нейрона, на втором слое использовано 32 нейрона, функции активации скрытых слоев tanh-relu, на выходном слое была использована функция softmax для распределения вероятностей определения класса новосных лент, на этом этапе была использована многокатегориальная перекрестная энтропия, как функция потерь, которая определяет расстояние между распределениями вероятностей. Для нахождения оптимального количество эпох обучения было выбрано сначало 30 эпох, однако результаты и графики показали, что этого было недостаточно, потом обучили на 50 эпохах, в итоге после 43 эпохи начилось переобучение модели. Обучив модель на 43 эпохах получили точность в 71%, при потерях 1.41. В итоге, классификатор _RandomForestClassifier_ проиграл в точности на 4.5%.\n",
    "\n",
    "3. В третьем этапе была решена задача регрессии, она выполняется с применением иных функций потерь, нежели классификация. В этой задаче Использовали набор данных boston_housing, с количеством 506 экземпляров. Обработали данные с помощью нормализации. Написали класс _HyperparameterTuningGenetic_ для получениия ошибки регресионной модели обученной на данных из _boston_housing_. По сравнению с предыдущим этапом были изменены следющие функции: метод класса _initReutersDataset_ изменился на _initBostonDataset_, он выгружает данные из набора данных _boston_housing_ и нормализует данные так как данные имеют самые разные диапазоны. Суть нормализации состоит в том что для каждого признака во входных данных (столбца в матрице входных данных) из каждого значения вычитается среднее по этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение, равное единице. Метод класса _Vectorize_sequences_ был удалён. Метод класса _convertParam_ как и _formatParams_ изменились только в количестве обработки гиперпараметров. Метод класса _getAccuracy_ поменял модель на _RandomForestRegressor_. Так как наша цель минимизировать оценку mae. Мы изменили стратегию: единственная цель – минмизировать стратегию приспособления _FitnessMin_, а веса _weights_ поставим отрицательными значениями. функция приспособленности класса _Individual_ в качестве атрибута станла _FitnessMin_. Так как в процессе тестирования _RandomForestRegressor_. Получили ошибку в 2537 долларов США, с гиперпараметрами _RandomForestRegressor_: _n_estimators = 209, criterion = squared_error, max_depth = 61, min_samples_split = 4, min_samples_leaf = 7, max_features = None, max_leaf_nodes = 20, bootstrap = True, ccp_alpha = 0.01909_. Попробовали поменять способ обучения регрессионной модели, используя метод перкрестной проверки по K блокам. Для этого в классе _HyperparameterTuningGenetic_ изменили функцию _getAccuracy_, использовали перекрестную проверку с помощью функции _cross_val_score_ и задали метрику потери _mean_absolute_error_. Получили ошибку в 2414 долларов США, что меньше по сравнению с предущим способом обучения, с гиперпараметрами _RandomForestRegressor_: _n_estimators = 500, criterion = friedman_mse, max_depth = 23, min_samples_split = 12, min_samples_leaf = 2, max_features = None, max_leaf_nodes = 19, bootstrap = True, ccp_alpha = 0.00116_. Сравнивая с моделью, которая была построена на втором этапе первой лабораторной работе, это сеть состяла из двух скрытых слоев, с функциями активации tanh и по 64 нейрона и заканчивается одномерным слоем (линейным слоем), не имеющим функции активации. Скомпелировали модель с парматрами: оптимизатор - rmsprop, функция потерь: mse (среднеквадратичная ошибка) и метрику оценки mae (средняя абсолютная ошибка). Из-за малого количества данных boston_housing решили надежно оценить качество модели с помощью метода перекрестной проверки по K блокам. Обучили модель на 500 эпохах, однако при оценке обучения из-за проблем с масштабированием, а также ввиду относительно высокой дисперсии затруднительно увидеть общую тенденцию. Для оптимизации были опущены первые 60 замеров, которые имеют другой масштаб, отличный от масштаба остальной кривой, а также каждая оценка была заменена экспоненциальным скользящим средним по предыдущим оценкам. В результате переобучили модель на 150 эпох, получая среднюю оценку ошибки прогнозирования, которая составляет около 2372 долларов США. В итоге, классификатор _RandomForestRegressor_ проиграл в прогнозировании на 42 доллара США.\n",
    "\n",
    "<p style=\"text-align: center;\">Список использованной литературы</p>\n",
    "    1. Шолле Франсуа. Глубокое обучение на Python. - СПб.: Питер, 2018. - 400 с.: ил. - (Серия «Библиотека программиста»).\n",
    "    2. Вирсански Э. - Генетические алгоритмы на Python / пер. с англ. А. А. Слинкина. – М.: ДМК Пресс, 2020. – 286 с.: ил.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
