{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Основные этапы работы:**\n",
    "## Этап 0. Установка и настройка оболочки для работы с языком Python\n",
    "\n",
    "В лабораторной работе использовалсь среда разработки VS Code с расширением Juputer <u>version: 2024.2.0</u>.\n",
    "\n",
    "Загрузка необходимых библиотек для выполнения лаборатрных работ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import  Embedding, GRU, Dense\n",
    "from keras.models import  Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1. Построение рекуррентной нейронной сети для распознавания эмоциональной окраски отзывов из базы данных IMDB.\n",
    "\n",
    "**Целью данного этапа** лабораторной работы является создание рекуррентного бинарного классификатора эмоциональной окраски отзывов из набора данных IMDB. Пошаговая реализация поставленной цели включает.\n",
    "\n",
    "Вариант этапа 1\n",
    "| № | Количество рекуррентных слоев | Размерность векторного представления | Количество нейронов на слое | Рекуррентный слой |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 3 | 1 | 32 | 128 | GRU |\n",
    "\n",
    " Пошаговая реализация поставленной цели включает:\n",
    " \n",
    "    1. Загрузка набора данных IMDB:\n",
    "База данных состоит из 50000 отзывов к кинолентам в интернет-базе (Internet Movie Database). Набор разбит на 25000 обучающих и 25 000 контрольных отзывов, каждый набор на 50 % состоит из отрицательных и на 50 % из положительных отзывов. Набор данных IMDB поставляется в составе Keras. Набор готов к использованию: отзывы (последовательности слов) преобразованы в последовательности целых чисел, каждое из которых определяет позицию слова в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных осуществляется из библиотеки Keras. Аргумент при загрузке указывает, что в обучающих данных будет сохранено только 10000 слов, наиболее часто встречающихся в обучающем наборе. Данные изначально разделены на тренировочные и тестовые в соотношении 1:1. Размер обучающего набора составляет 25000 экземпляров.\n",
    "\n",
    "    2. Создание векторного представления слов из набора данных;\n",
    "    3. Разделение данных на обучающий и тестовый наборы:\n",
    "Аргумент _num_words=max_features_ означает, что в обучающих данных будет сохранено только 10000 слов, наиболее часто встречающихся в обучающем наборе отзывов, остальные слова будут отброшены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что из себя представляют данные на примере первого отзыва:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменные _input_train_ и _input_test_ — это списки отзывов; каждый отзыв — это список индексов слов (кодированное представление последовательности слов). Для декодирования данных и получения текста воспользуемся вспомогательной функцией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert . is an amazing actor and now the same being director . father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for . and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also . to the two little boy's that played the . of norman and paul they were just brilliant children are often left out of the . list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "\n",
    "reverse_index = dict([(value,key) for (key, value) in index.items()])\n",
    "decoder = ' '.join([reverse_index.get(i-3,'.') for i in input_train[0]])\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменные _y_train_ и _y_test_ — это списки нулей и единиц, где нули соответствуют отрицательным отзывам, а единицы — положительным.\n",
    "\n",
    "    4. Подготовка данных для передачи в нейронную сеть\n",
    "Преобразуем списки целых чисел в двумерный тензор с целыми числами и с формой (образцы, максимальная_длина) с помощью функции _pad_sequences_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 3586,    2],\n",
       "       [   0,    0,    0, ...,   12,    9,   23],\n",
       "       [   0,    0,    0, ...,  204,  131,    9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "input_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Конструирование сети: создание сети из слоя Embedding и рекуррентных слоев заданного типа в соответствии с вариантом;\n",
    "    6. Настройка оптимизатора с выбором функции потерь и метрики качества. Число эпох принять от 30 до 60:\n",
    "\n",
    "С помощью функции _Sequential()_ создаём новую модель. Создание модели реализовано при помощи функции _create_model_, она включает в себя конструирование модели, настройку оптимизатора и выбор функций потерь и качества. Первый слой, _Embedding(num_words, 32, input_length=maxlen)_, слой встраивания, который преобразует индексы слов в dense-векторы размерности 32. _num_words_ - это количество уникальных слов в словаре, а _maxlen_ - максимальная длинна входных последовательностей. Второй слой _GRU_, рекуррентный слой c 128 нейронами. Последний слой состоит из одного нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность.\n",
    "\n",
    "Слой Embedding это как словарь, отображающий целочисленные индексы (обозначающие конкретные слова) в плотные векторы. Он принимает целые числа на входе, отыскивает их во внутреннем словаре и возвращает соответствующие векторы. Это эффективная операция поиска в словаре (рис. 1). Слой Embedding получает на входе двумерный тензор с целыми числами, каждый элемент которого является последовательностью целых чисел. Он может работать с последовательностями разной длины. Этот слой возвращает трехмерный тензор с вещественными числами и с формой (образцы, длина_последовательности, размерность_векторного_представления). Такой трехмерный тензор можно затем обработать слоем RNN или одномерным сверточным слоем. При создании слоя Embedding, его веса (внутренний словарь векторов токенов) инициализируются случайными значениями. В процессе обучения векторы слов постепенно корректируются посредством обратного распространения ошибки, и пространство превращается в структурированную модель, пригодную к использованию. После полного обучения пространство векторов приобретет законченную структуру, специализированную под решение конкретной задачи.\n",
    "\n",
    "<center><img src=\"3.1.6.png\"></center>\n",
    "<center>Рис. 1. Слой Embedding</center>\n",
    "\n",
    "Рекуррентная нейронная сеть (Recurrent Neural Network, RNN) — это разновидность нейронной сети, имеющей внутренний цикл (рис. 2). Она обрабатывает последовательность, перебирая ее элементы и сохраняя состояние, полученное при обработке предыдущих элементов. Сеть RNN сбрасывает состояние между обработкой двух разных, независимых последовательностей (таких, как два разных отзыва из IMDB), поэтому одна последовательность все еще интерпретируется как единый блок данных: единственный входной пакет. Однако блок данных обрабатывается не за один шаг; сеть выполняет внутренний цикл, перебирая последовательность элементов. _RNN_ это цикл _for_, который повторно использует величины, вычисленные в предыдущей итерации.\n",
    "\n",
    "<center><img src=\"3.1.5.png\"></center>\n",
    "<center>Рис. 2. Рекуррентная сеть — сеть с циклом</center>\n",
    "\n",
    "Согласно варианту используем рекурретный слой GRU. GRU (Gated Recurrent Unit), является одним из видов рекуррентных нейронных сетей (RNN), аналогичным LSTM (Long Short-Term Memory). GRU решает проблему с затуханием градиента. Суть работы слоя LSTM: он сохраняет информацию для последующего использования, тем самым предотвращая постепенное затухание старых сигналов во время обработки. Слои управляемых рекуррентных блоков GRU основаны на том же принципе, что и слои LSTM, однако они представляют собой более простые структуры и, соответственно, менее затратны в вычислительном смысле (хотя могут не иметь такой же репрезентативной мощности, как LSTM). Этот компромисс между затратностью вычислений и репрезентативной мощностью можно наблюдать повсюду в области машинного обучения.\n",
    "\n",
    "<center><img src=\"3.1.77.png\"></center>\n",
    "<center>Рис. 3. Работа ячейки GRU</center>\n",
    "\n",
    "Здесь $u_t$ — это гейт обновления (update gate), который и является комбинацией\n",
    "входного и забывающего гейтов. А $r_t$ — это гейт перезагрузки (reset gate); он тоже\n",
    "отвечает за то, какую часть памяти нужно перенести дальше с прошлого шага, но\n",
    "делает это еще до применения нелинейной функции. Ячейка памяти и выход блока\n",
    "$h_t$ тут, в отличие от LSTM, никак не разделяются, и следующий выход $h_t$ получается как комбинация (задаваемая гейтом $u_t$) предыдущего выхода $h_t−1$ и текущего\n",
    "кандидата в выход $h^{′}_t$ , который, в свою очередь, тоже зависит от $h_t−1$, но на этот раз через гейт перезагрузки $r_t$.\n",
    "\n",
    "<center><img src=\"3.1.8.png\"></center>\n",
    "<center>Рис. 4. Структура ячейки GRU</center>\n",
    "\n",
    "Основная разница между GRU и LSTM состоит в том, что GRU пытается сделать двумя гейтами то же самое, что LSTM делает тремя. Обязанности забывающего гейта f в LSTM здесь разделены между двумя гейтами, t и $u_t$. Кроме того, не возникает второй нелинейности на пути от входа к выходу, как в случае LSTM. Заметим еще, что здесь опять нужно правильно проинициализировать свободные члены в гейте обновления $u_t$: свободные члены $b_u$ должны быть большими, иначе опять возникнет нежелательный эффект с экспоненциальным затуханием «памяти» в последовательности GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32, input_length = maxlen))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекрестная энтропия (crossentropy) - это мера расстояния между распределениями вероятностей, или в данном случае - между фактическими данными и предсказаниями.\n",
    "\n",
    "Настраиваем модель оптимизатором rmsprop и функцией потерь mеап squared error.\n",
    "\n",
    "rmsprop - наиболее подходящий оптимизатор, популярный в использовании для большинства нейронных сетей.\n",
    "\n",
    "Среднеквадратичное распространение корня (RMSprop) - это экспоненциально затухающее среднее значение. Существенным свойством RMSprop является то, что вы не ограничены только суммой прошлых градиентов, но вы более ограничены градиентами последних временных шагов. В RMSProp мы пытаемся уменьшить вертикальное движение, используя среднее значение, потому что они суммируются приблизительно до 0, принимая среднее значение. RMSprop предоставляет среднее значение для обновления. Формула обновления изображена на рисунке 5.\n",
    "\n",
    "<center><img src=\"2.1.8.png\"></center>\n",
    "<center>Рис. 5. Формула rmsprop</center>\n",
    "\n",
    "    7. Проведение проверки решения, выделяя контрольное множество:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "157/157 [==============================] - 78s 488ms/step - loss: 0.6574 - acc: 0.6047 - val_loss: 0.5283 - val_acc: 0.7460\n",
      "Epoch 2/60\n",
      "157/157 [==============================] - 79s 502ms/step - loss: 0.4270 - acc: 0.8051 - val_loss: 0.4374 - val_acc: 0.8042\n",
      "Epoch 3/60\n",
      "157/157 [==============================] - 79s 503ms/step - loss: 0.3403 - acc: 0.8615 - val_loss: 0.3838 - val_acc: 0.8268\n",
      "Epoch 4/60\n",
      "157/157 [==============================] - 79s 504ms/step - loss: 0.2969 - acc: 0.8794 - val_loss: 0.3443 - val_acc: 0.8660\n",
      "Epoch 5/60\n",
      "157/157 [==============================] - 80s 509ms/step - loss: 0.2470 - acc: 0.9032 - val_loss: 0.3436 - val_acc: 0.8500\n",
      "Epoch 6/60\n",
      "157/157 [==============================] - 80s 510ms/step - loss: 0.2221 - acc: 0.9159 - val_loss: 0.3426 - val_acc: 0.8572\n",
      "Epoch 7/60\n",
      "157/157 [==============================] - 78s 494ms/step - loss: 0.1995 - acc: 0.9233 - val_loss: 0.3953 - val_acc: 0.8338\n",
      "Epoch 8/60\n",
      "157/157 [==============================] - 79s 504ms/step - loss: 0.1814 - acc: 0.9338 - val_loss: 0.3535 - val_acc: 0.8732\n",
      "Epoch 9/60\n",
      "157/157 [==============================] - 78s 498ms/step - loss: 0.1609 - acc: 0.9402 - val_loss: 0.3458 - val_acc: 0.8558\n",
      "Epoch 10/60\n",
      "157/157 [==============================] - 80s 508ms/step - loss: 0.1494 - acc: 0.9463 - val_loss: 0.4182 - val_acc: 0.8450\n",
      "Epoch 11/60\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 0.1379 - acc: 0.9512 - val_loss: 0.3644 - val_acc: 0.8652\n",
      "Epoch 12/60\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 0.1243 - acc: 0.9560 - val_loss: 0.4290 - val_acc: 0.8734\n",
      "Epoch 13/60\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 0.1131 - acc: 0.9592 - val_loss: 0.3716 - val_acc: 0.8480\n",
      "Epoch 14/60\n",
      "157/157 [==============================] - 82s 519ms/step - loss: 0.1006 - acc: 0.9669 - val_loss: 0.4845 - val_acc: 0.8552\n",
      "Epoch 15/60\n",
      "157/157 [==============================] - 81s 517ms/step - loss: 0.0924 - acc: 0.9683 - val_loss: 0.4387 - val_acc: 0.8444\n",
      "Epoch 16/60\n",
      "157/157 [==============================] - 78s 496ms/step - loss: 0.0818 - acc: 0.9733 - val_loss: 0.4615 - val_acc: 0.8706\n",
      "Epoch 17/60\n",
      "157/157 [==============================] - 81s 513ms/step - loss: 0.0714 - acc: 0.9740 - val_loss: 0.5509 - val_acc: 0.8654\n",
      "Epoch 18/60\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.5446 - val_acc: 0.8674\n",
      "Epoch 19/60\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 0.0565 - acc: 0.9815 - val_loss: 0.5993 - val_acc: 0.8670\n",
      "Epoch 20/60\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 0.0508 - acc: 0.9836 - val_loss: 0.5684 - val_acc: 0.8646\n",
      "Epoch 21/60\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 0.0433 - acc: 0.9864 - val_loss: 0.6333 - val_acc: 0.8656\n",
      "Epoch 22/60\n",
      "157/157 [==============================] - 82s 524ms/step - loss: 0.0379 - acc: 0.9876 - val_loss: 0.5593 - val_acc: 0.8428\n",
      "Epoch 23/60\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 0.0347 - acc: 0.9888 - val_loss: 0.5611 - val_acc: 0.8538\n",
      "Epoch 24/60\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.6002 - val_acc: 0.8550\n",
      "Epoch 25/60\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 0.0279 - acc: 0.9912 - val_loss: 0.6932 - val_acc: 0.8634\n",
      "Epoch 26/60\n",
      "157/157 [==============================] - 81s 514ms/step - loss: 0.0249 - acc: 0.9924 - val_loss: 0.6835 - val_acc: 0.8580\n",
      "Epoch 27/60\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 0.0256 - acc: 0.9926 - val_loss: 0.9291 - val_acc: 0.8238\n",
      "Epoch 28/60\n",
      "157/157 [==============================] - 82s 523ms/step - loss: 0.0216 - acc: 0.9941 - val_loss: 0.9619 - val_acc: 0.8204\n",
      "Epoch 29/60\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 0.0213 - acc: 0.9933 - val_loss: 0.7715 - val_acc: 0.8574\n",
      "Epoch 30/60\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 0.0162 - acc: 0.9955 - val_loss: 0.8830 - val_acc: 0.8370\n",
      "Epoch 31/60\n",
      "157/157 [==============================] - 80s 512ms/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.8057 - val_acc: 0.8646\n",
      "Epoch 32/60\n",
      "157/157 [==============================] - 82s 521ms/step - loss: 0.0137 - acc: 0.9959 - val_loss: 0.8231 - val_acc: 0.8636\n",
      "Epoch 33/60\n",
      "157/157 [==============================] - 83s 526ms/step - loss: 0.0105 - acc: 0.9971 - val_loss: 0.8289 - val_acc: 0.8548\n",
      "Epoch 34/60\n",
      "157/157 [==============================] - 82s 520ms/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.8170 - val_acc: 0.8554\n",
      "Epoch 35/60\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 0.0131 - acc: 0.9957 - val_loss: 0.7593 - val_acc: 0.8522\n",
      "Epoch 36/60\n",
      "157/157 [==============================] - 81s 516ms/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.9009 - val_acc: 0.8570\n",
      "Epoch 37/60\n",
      "157/157 [==============================] - 81s 515ms/step - loss: 0.0109 - acc: 0.9973 - val_loss: 0.9199 - val_acc: 0.8614\n",
      "Epoch 38/60\n",
      "157/157 [==============================] - 80s 513ms/step - loss: 0.0102 - acc: 0.9974 - val_loss: 0.9310 - val_acc: 0.8604\n",
      "Epoch 39/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0093 - acc: 0.9972 - val_loss: 0.9432 - val_acc: 0.8630\n",
      "Epoch 40/60\n",
      "157/157 [==============================] - 79s 505ms/step - loss: 0.0110 - acc: 0.9977 - val_loss: 0.9098 - val_acc: 0.8564\n",
      "Epoch 41/60\n",
      "157/157 [==============================] - 78s 499ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.8822 - val_acc: 0.8600\n",
      "Epoch 42/60\n",
      "157/157 [==============================] - 79s 503ms/step - loss: 0.0096 - acc: 0.9979 - val_loss: 0.9847 - val_acc: 0.8196\n",
      "Epoch 43/60\n",
      "157/157 [==============================] - 79s 502ms/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.8488 - val_acc: 0.8548\n",
      "Epoch 44/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0100 - acc: 0.9978 - val_loss: 0.8828 - val_acc: 0.8608\n",
      "Epoch 45/60\n",
      "157/157 [==============================] - 80s 509ms/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.9797 - val_acc: 0.8584\n",
      "Epoch 46/60\n",
      "157/157 [==============================] - 80s 510ms/step - loss: 0.0088 - acc: 0.9981 - val_loss: 1.0516 - val_acc: 0.8658\n",
      "Epoch 47/60\n",
      "157/157 [==============================] - 79s 503ms/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.9212 - val_acc: 0.8568\n",
      "Epoch 48/60\n",
      "157/157 [==============================] - 80s 510ms/step - loss: 0.0079 - acc: 0.9986 - val_loss: 1.0082 - val_acc: 0.8644\n",
      "Epoch 49/60\n",
      "157/157 [==============================] - 80s 508ms/step - loss: 0.0064 - acc: 0.9987 - val_loss: 1.0256 - val_acc: 0.8648\n",
      "Epoch 50/60\n",
      "157/157 [==============================] - 81s 518ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 1.0172 - val_acc: 0.8616\n",
      "Epoch 51/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.9725 - val_acc: 0.8586\n",
      "Epoch 52/60\n",
      "157/157 [==============================] - 80s 507ms/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.9037 - val_acc: 0.8620\n",
      "Epoch 53/60\n",
      "157/157 [==============================] - 80s 509ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 1.0347 - val_acc: 0.8672\n",
      "Epoch 54/60\n",
      "157/157 [==============================] - 80s 508ms/step - loss: 0.0059 - acc: 0.9989 - val_loss: 0.9432 - val_acc: 0.8372\n",
      "Epoch 55/60\n",
      "157/157 [==============================] - 79s 506ms/step - loss: 0.0032 - acc: 0.9993 - val_loss: 1.0735 - val_acc: 0.8562\n",
      "Epoch 56/60\n",
      "157/157 [==============================] - 79s 506ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 1.1079 - val_acc: 0.8680\n",
      "Epoch 57/60\n",
      "157/157 [==============================] - 80s 509ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 1.0779 - val_acc: 0.8664\n",
      "Epoch 58/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 1.0062 - val_acc: 0.8638\n",
      "Epoch 59/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 1.0187 - val_acc: 0.8630\n",
      "Epoch 60/60\n",
      "157/157 [==============================] - 80s 511ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.9633 - val_acc: 0.8354\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history = model.fit(input_train, y_train,\n",
    "    epochs=60,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Вывод графиков функции потерь и точности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.1.1.png\"></center>\n",
    "<center>Рис. 6. Точность на этапах обучения и проверки</center>\n",
    "<center><img src=\"3.1.2.png\"></center>\n",
    "<center>Рис. 7. Потери на этапах обучения и проверки</center>\n",
    "    \n",
    "    9. Использование обученной сети для предсказания на новых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 29s 37ms/step - loss: 1.0218 - acc: 0.8314\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(input_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогам обучения с 60 эпохами точность достигла 83%.\n",
    "\n",
    "На этапе обучения потери снижаются с каждой эпохой, а точность растет. Это ожидаемое поведение от оптимизации градиентным спуском: величина, которую необходимо минимизировать, становиться все меньше с каждой итерацией на обучающих данных. Однако потери и точность на этапе проверки достигает пика в третью эпоху. Далее происходит переобучение: после третьей эпохи произошла чрезмерная оптимизация на обучающих данных, и в результате получилось представление, характерное для обучающих данных, не обобщающее данные за пределами обучающего набора.\n",
    "\n",
    "В данном случае для предотвращения переобучения можно прекратить обучение после 8 эпохи.\n",
    "\n",
    "Скомпилируем заново модель, ограничимся 8 эпохами и проверим точность на контрольных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "157/157 [==============================] - 78s 489ms/step - loss: 0.6336 - acc: 0.6154 - val_loss: 0.5393 - val_acc: 0.7300\n",
      "Epoch 2/8\n",
      "157/157 [==============================] - 74s 474ms/step - loss: 0.4239 - acc: 0.8128 - val_loss: 0.4447 - val_acc: 0.8040\n",
      "Epoch 3/8\n",
      "157/157 [==============================] - 75s 477ms/step - loss: 0.3551 - acc: 0.8594 - val_loss: 0.3586 - val_acc: 0.8564\n",
      "Epoch 4/8\n",
      "157/157 [==============================] - 75s 478ms/step - loss: 0.2969 - acc: 0.8791 - val_loss: 0.3844 - val_acc: 0.8326\n",
      "Epoch 5/8\n",
      "157/157 [==============================] - 75s 477ms/step - loss: 0.2532 - acc: 0.9004 - val_loss: 0.3913 - val_acc: 0.8332\n",
      "Epoch 6/8\n",
      "157/157 [==============================] - 76s 482ms/step - loss: 0.2221 - acc: 0.9160 - val_loss: 0.3232 - val_acc: 0.8754\n",
      "Epoch 7/8\n",
      "157/157 [==============================] - 75s 478ms/step - loss: 0.2008 - acc: 0.9238 - val_loss: 0.3882 - val_acc: 0.8638\n",
      "Epoch 8/8\n",
      "157/157 [==============================] - 76s 481ms/step - loss: 0.1817 - acc: 0.9316 - val_loss: 0.3436 - val_acc: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26770205bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(input_train, y_train,\n",
    "    epochs=8,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 28s 36ms/step - loss: 0.3458 - acc: 0.8548\n"
     ]
    }
   ],
   "source": [
    "results_end = model.evaluate(input_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обученная в течении 8 эпох показала точность в 85.5% при потерях 0.3458.\n",
    "\n",
    "Посмотрим сколько мы выигрываем при обучении модели на двадцати эпохах и модели обученной на трех эпохах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6759388744831085\n",
      "0.023320019245147705\n"
     ]
    }
   ],
   "source": [
    "print(results_end[0] - results[0])\n",
    "print(results_end[1] - results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно увидеть, мы выйграли не только время обучения сети, но и 0.02 точности и 0.68 потери. \n",
    "    \n",
    "    10. Сопоставление полученных результатов с одномерной сверточной сетью (три сверточных слоя).\n",
    "\n",
    "Для оценки качества работы модели на основе реккурентной сети можем использовать сравнение с одномерной сверточной сетью.\n",
    "\n",
    "Одномерные сверточные нейронные сети могут состязаться с рекуррентными сетями в некоторых задачах обработки последовательностей, как правило, требуя меньше вычислительных ресурсов. Небольшие одномерные сверточные нейронные сети могут служить быстрой альтернативой рекуррентным сетям в простых задачах, таких как классификация текста и прогнозирование временных последовательностей. \n",
    "\n",
    "Одномерные сверточные слои способны распознавать локальные шаблоны в последовательности. Поскольку к каждому шаблону применяются одни и те же преобразования, тот или иной шаблон, найденный в некоторой позиции в предложении, позднее может быть опознан в другой позиции, что делает преобразования, выполняемые одномерными сверточными сетями, инвариантными (во времени)\n",
    "\n",
    "<center><img src=\"3.1.9.png\"></center>\n",
    "<center>Рис. 8. Иллюстрации принципа одномерной свертки</center>\n",
    "\n",
    "С помощью функции _Sequential()_ создаём новую модель. Первый слой, _Embedding(num_words, 32, input_length=maxlen)_, слой встраивания, который преобразует индексы слов в dense-векторы размерности 32, аналогичен с предыдущей модели. Второй слой Conv1D с 32 фильтрами размера 7 и функцией активации _relu_. Этот слой извлекает локальные признаки из входных векторов. Затем добавлен MaxPooling1D(5): слой максимального объединения, который уменьшает размерность выходных данных сверточного слоя путем выбора максимального значения из окна размера 5. Следующий слой анологичен со вторым слоем, который извлекает более высокоуровневые признаки из текстовых данных. Предпоследний слой _GlobalMaxPooling1D()_, слой глобального максимального объединения, который выбирает максимальное значение из каждого канала, что приводит к одному вектору признаков для всего входного текста. Последний слой состоит из одного нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Он выдает вероятность принадлежности входного текста к положительному классу.\n",
    "\n",
    "После определения архитектуры модель компилируется с помощью функции model.compile(). Используется оптимизатор RMSprop для обновления весов модели во время обучения. Функция потерь бинарной кросс-энтропии, подходящая для задачи бинарной классификации. Метрика точности будет отслеживаться во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 44ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 6s 40ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 6s 40ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 6s 38ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'])\n",
    "history2 = model.fit(input_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0862 - acc: 0.8646\n"
     ]
    }
   ],
   "source": [
    "results_end3 = model.evaluate(input_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем графики потери и точности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history2.history['acc']\n",
    "val_acc = history2.history['val_acc']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.1.3.png\"></center>\n",
    "<center>Рис. 9. Точность на этапах обучения и проверки</center>\n",
    "<center><img src=\"3.1.4.png\"></center>\n",
    "<center>Рис. 10. Потери на этапах обучения и проверки</center>\n",
    "\n",
    "По итогам обучения с 10 эпохами точность достигла 86%.\n",
    "\n",
    "Первая модель показывает точность 85% при потерях в 0.34, вторая 86% при потерях в 1. Как видно точность прогнозирования почти индентичны, но потери одномерной сверточной сети значительно больше. \n",
    "\n",
    "Пример работы обученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". please give this one a miss br br . . and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite . so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "\n",
    "reverse_index = dict([(value,key) for (key, value) in index.items()])\n",
    "decoder = ' '.join([reverse_index.get(i-3,'.') for i in input_test[0]])\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating(x):\n",
    "    if x>0.5:\n",
    "        return \"положительный отзыв\"\n",
    "    else:\n",
    "        return \"отрицательный отзыв\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "Predict:  отрицательный отзыв\n",
      "True:  отрицательный отзыв\n"
     ]
    }
   ],
   "source": [
    "ex = input_test[0]\n",
    "ex = np.expand_dims(ex, axis=0)\n",
    "pred = model.predict(ex)[0]\n",
    "print('Predict: ',rating(pred))\n",
    "print('True: ',rating(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель первый из тестовых отзывов оценила как отрицательный отзыв. Провелили в списках меток. Они совпали"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2. Построение прогноза температуры с использованием рекуррентных и сверточных нейронных сетей.\n",
    "\n",
    "**Целью данного этапа** лабораторной работы является создание нейронной сети, реализующей прогноз температуры по набору данных Jena Climate.\n",
    "\n",
    "Вариант этапа 2\n",
    "| № | Количество рекуррентных слоев | Количество нейронов на слое | Рекуррентный слой | Прореживание |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 3 | 2 | 64-32 | GRU (двунаправленная сеть) | 0.2 |\n",
    "\n",
    " Пошаговая реализация поставленной цели включает:\n",
    "\n",
    "1. Загрузка набора данных Jena Climate:\n",
    "\n",
    "В набор данных включены замеры 14 разных характеристик (таких, как температура, атмосферное давление, влажность, направление ветра и т. д.), выполнявшиеся каждые 10 минут в течение нескольких лет,  в этот пример включены только данные за 2009–2016 годы. Данные изначально не разделены на тренировочные и тестовые наборы. Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/svyatoslav/Desktop/магистратура/2 курс 4 семестр/Спецкурс по Нейронным сетям/jena_climate_2009_2016.csv\"\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные в массив Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    " values = [float(x) for x in line.split(',')[1:]]\n",
    " float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предварительного анализа и понимания характера данных перед их использованием для обучения нейронной сети можно осуществить визуализацию временных рядов признаков. \n",
    "Визуализация позволяет быстро оценить:\n",
    "*\tНаличие пропущенных значений или выбросов в данных.\n",
    "*\tХарактер распределения признаков (периодичность, тренды, сезонность и т.д.).\n",
    "*\tДиапазоны значений для различных признаков.\n",
    "*\tВзаимосвязь между признаками (если они коррелируют, то графики могут иметь схожий характер).\n",
    "\n",
    "Визуализируем временные ряды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_grafs = [\"p (mbar)\",\"T (degC)\",\"Tpot (K)\",\"Tdew (degC)\",\"rh (%)\",\"VPmax (mbar)\",\"VPact (mbar)\",\"VPdef (mbar)\",\"sh (g/kg)\",\"H2OC (mmol/mol)\",\"rho (g/m**3)\",\"wv (m/s)\",\"max. wv (m/s)\",\"wd (deg)\"]\n",
    "for i in range(len(keys_grafs)):\n",
    "    temp = float_data[:, i] # температура (в градусах Цельсия)\n",
    "    plt.plot(range(len(temp)), temp)\n",
    "    plt.title(keys_grafs[i]) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.2.1.png\"></center>\n",
    "<center><img src=\"3.2.2.png\"></center>\n",
    "<center><img src=\"3.2.3.png\"></center>\n",
    "<center><img src=\"3.2.4.png\"></center>\n",
    "<center><img src=\"3.2.5.png\"></center>\n",
    "<center><img src=\"3.2.6.png\"></center>\n",
    "<center><img src=\"3.2.7.png\"></center>\n",
    "<center><img src=\"3.2.8.png\"></center>\n",
    "<center><img src=\"3.2.9.png\"></center>\n",
    "<center><img src=\"3.2.10.png\"></center>\n",
    "<center><img src=\"3.2.11.png\"></center>\n",
    "<center><img src=\"3.2.12.png\"></center>\n",
    "<center><img src=\"3.2.13.png\"></center>\n",
    "<center><img src=\"3.2.14.png\"></center>\n",
    "<center>Рис. 11. Графики зависимости признаков от времени</center>\n",
    "\n",
    "Анализируя данные графики можем сделать следующие заключения:\n",
    "*\tМногие признаки демонстрируют явную сезонность и периодические колебания, что характерно для климатических данных.\n",
    "*\tРазные признаки имеют различные диапазоны значений. \n",
    "\n",
    "    Это указывает на необходимость нормализации или масштабирования данных перед обучением модели машинного обучения.\n",
    "*\tМожно заметить несколько потенциальных выбросов, особенно в данных о скорости ветра и давлении.\n",
    "*\tНекоторые признаки, такие как температура, точка росы и влажность, демонстрируют схожие паттерны, что может указывать на их взаимосвязь и корреляцию.\n",
    "*\tМногие признаки демонстрируют явную годовую периодичность, что соответствует сезонным изменениям климата.\n",
    "\n",
    "    3. Создание генератора данных, выбрав базу данных за 10 дней, задержку в 1 день, 1 значение в час и размер пакета из варианта:\n",
    "\n",
    "_generator_ — функция-генератор данных. Она возвращает кортеж (образцы, цели), где образцы — это один пакет входных данных, а цели — соответствующий массив целевых температур. Функция принимает следующие аргументы:\n",
    "\n",
    "* data — исходный массив вещественных чисел, который будет нормализоваться дальше;\n",
    "* lookback — количество интервалов в прошлом от заданного момента, за которое отбираются входные данные;\n",
    "* delay — количество интервалов в будущем от заданного момента, за которое отбираются целевые данные;\n",
    "* min_index и max_index — индексы в массиве data, ограничивающие область для извлечения данных; это помогает оставить в неприкосновенности сегменты проверочных и контрольных данных;\n",
    "* shuffle — флаг, определяющий порядок извлечения образцов: с перемешиванием или в хронологическом порядке;\n",
    "* batch_size — количество образцов в пакете;\n",
    "* step — период в интервалах, из которого извлекается один образец; мы установим его равным 6, чтобы получить по одному образцу за каждый час."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "    shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                            lookback // step,\n",
    "                            data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Подготовка данных для передачи в нейронную сеть:\n",
    "\n",
    "В процессе обработки данных мы будем вычитать среднее для каждой временной последовательности и делить на стандартное отклонение. Для обучения мы используем первые 200000 замеров, поэтому среднее и стандартное отклонение должны вычисляться только по этой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя функцию-генератор _generator_ для создания трех других функций-генераторов (train_gen, val_gen и test_gen): для получения обучающих, проверочных и контрольных данных. Все они будут отбирать образцы из разных временных сегментов оригинальных данных: обучающие данные будут извлекаться из первых 200 000 интервалов, проверочные — из следующих 100 000, а контрольные — из остальных. бучающий генератор (train_gen) использует данные с индексами от 0 до 200000. Валидационный генератор (val_gen) использует данные с индексами от 200001 до 300000. Тестовый генератор (test_gen) использует данные с индексами после 300001. Переменные val_steps и test_steps вычисляют количество шагов (итераций) для валидационного и тестового генераторов соответственно.\n",
    "\n",
    "* lookback — определяет длину истории (временной ряд из прошлых значений);\n",
    "* delay — смещение до целевого значения;\n",
    "* step — управляет шагом выборки данных из временного ряда. \n",
    "* shuffle — определяет, нужно ли перемешивать данные перед выборкой. \n",
    "* batch_size — задает размер порции данных, возвращаемой генератором за один вызов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "train_gen = generator(float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=0,\n",
    "    max_index=200000,\n",
    "    shuffle=True,\n",
    "    step=step,\n",
    "    batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=200001,\n",
    "    max_index=300000,\n",
    "    step=step,\n",
    "    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=300001,\n",
    "    max_index=None,\n",
    "    step=step,\n",
    "    batch_size=batch_size)\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Выполнить конструирование сети, создание сети рекуррентных слоев заданного типа в соответствии с вариантом:\n",
    "\n",
    "С помощью функции _Sequential()_ создаём новую модель. Первый слой, _Bidirectional_, слой встраивания, который в своем первом аргументе принимает экземпляр рекуррентного\n",
    "слоя _GRU_ с 64 нейронами, получает на вход временные последовательности данных с любым количеством временных шагов _(None)_ и размерностью признаков, равной размерности исходных данных _(values.shape[-1])_, параметры _dropout_ и _recurrent_dropout_ применяют регуляризацию отсечением для предотвращения переобучения, параметр _return_sequences_ указывает, что слой должен возвращать полную последовательность выходов, а не только выход на последнем временном шаге. Слой Bidirectional создает второй, отдельный экземпляр этого рекуррентного слоя и использует один экземпляр для обработки входных последовательностей в прямом порядке, а другой — в обратном. Второй слой _Bidirectional_ такой же как и первый, только рекуррентный слой GRU c 32 нейронами, он не возвращает полную последовательность выходов, а только выход на последнем временном шаге. Последний сдлой идет полносвязный слой (Dense) с одним нейроном для выполнения задачи регрессии (предсказания одного целевого значения).\n",
    "\n",
    "Слой Bidirectional создает второй, отдельный экземпляр этого рекуррентного слоя и использует один экземпляр для обработки входных последовательностей\n",
    "в прямом порядке, а другой — в обратном.\n",
    "\n",
    "В RNN есть ещё один тип сетей — это двунаправленные рекуррентные сети. Часто бывает так, что RNN к концу последовательности уже забывают, с чего все начиналось, также последние элементы последовательности, даже если не забудем начало, всегда будут гораздо важнее первых и в обычной RNN, и в сети из LSTM или GRU-ячеек. Поэтому часто рассматривают так называемые двунаправленные рекуррентные сети (bidirectional RNN). \n",
    "\n",
    "Проиллюстрируем на рис. 11 структуру двунаправленной нейронной сети. На этой схеме мы «спрятали» матрицы $W$ и $U$ в один блок и сконцентрировались на том, чтобы детально показать, что происходит с выходами; связи, относящиеся ко идущей справа налево рекуррентной сети, на рис. 11 показаны пунктиром. Формально говоря, в двунаправленной сети мы вычисляем состояния $s_t$ слева направо и состояния $s^{′}_t$ справа налево, а затем сливаем их в один результат уже на уровне выхода; это значит, что выход вычисляется как\n",
    "\n",
    "<center><img src=\"3.2.16.png\"></center>\n",
    "<center>Рис. 12. Вычисление выхода из двунаправленной рекуррентной сети</center>\n",
    "\n",
    "<center><img src=\"3.2.115.png\"></center>\n",
    "<center>Рис. 13. Двунаправленная рекуррентная сеть</center>\n",
    "\n",
    "Двунаправленная рекуррентная сеть использует чувствительность RNN к порядку: она состоит из двух обычных рекуррентных сетей, таких как слои GRU и LSTM. Обрабатывая последовательность в двух направлениях, двунаправленная рекуррентная сеть способна выявить шаблоны, незаметные для однонаправленной сети.  Она просматривает входную последовательность в обоих направлениях (рис. 14), получает потенциально более насыщенные представления и выделяет шаблоны, которые могли быть упущены однонаправленной версией. Двунаправленные рекуррентные сети, просматривающие последовательность данных в обоих направлениях, дают хорошие результаты в задачах обработки естественного языка. Но они мало пригодны для обработки последовательностей, в которых недавние данные информативнее, чем находящиеся в начале.\n",
    "\n",
    "<center><img src=\"3.2.17.png\"></center>\n",
    "<center>Рис. 14. Принцип действия двунаправленной рекуррентной нейронной сети</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(64, return_sequences=True, recurrent_dropout=0.2, dropout=0.2), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Bidirectional(layers.GRU(32, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Настройка оптимизатора с выбором функции потерь и метрики качества. Число эпох принять от 30 до 60\n",
    "\n",
    "Скомпелировали модель с помощью функции _compile_. Модель компилируется с использованием функции потерь mae (среднее абсолютное отклонение) и оптимизатора RMSprop. Функция потерь mae вычисляет среднее абсолютное отклонение между предсказанными и истинными значениями, что подходит для задач регрессии. Оптимизатор RMSprop (Root Mean Square Propagation) представляет собой адаптивный метод оптимизации, который регулирует скорость обучения в зависимости от истории градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Проведение проверки решения, выделяя контрольное множество:\n",
    "\n",
    "Обучим модель на 30 эпохах с шагом валидации в 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "500/500 [==============================] - 850s 2s/step - loss: 0.3133 - val_loss: 0.2760\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 823s 2s/step - loss: 0.2924 - val_loss: 0.2776\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 825s 2s/step - loss: 0.2850 - val_loss: 0.2753\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 840s 2s/step - loss: 0.2784 - val_loss: 0.2810\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 843s 2s/step - loss: 0.2723 - val_loss: 0.2737\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 846s 2s/step - loss: 0.2662 - val_loss: 0.2946\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 855s 2s/step - loss: 0.2579 - val_loss: 0.2867\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 845s 2s/step - loss: 0.2493 - val_loss: 0.2925\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 843s 2s/step - loss: 0.2411 - val_loss: 0.2951\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 838s 2s/step - loss: 0.2329 - val_loss: 0.2993\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 842s 2s/step - loss: 0.2234 - val_loss: 0.3159\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 854s 2s/step - loss: 0.2153 - val_loss: 0.3108\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 842s 2s/step - loss: 0.2103 - val_loss: 0.3222\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 831s 2s/step - loss: 0.2026 - val_loss: 0.3195\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 832s 2s/step - loss: 0.1967 - val_loss: 0.3157\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 840s 2s/step - loss: 0.1904 - val_loss: 0.3220\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 851s 2s/step - loss: 0.1847 - val_loss: 0.3238\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 1024s 2s/step - loss: 0.1800 - val_loss: 0.3388\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 1428s 3s/step - loss: 0.1751 - val_loss: 0.3325\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 1429s 3s/step - loss: 0.1725 - val_loss: 0.3373\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 1425s 3s/step - loss: 0.1671 - val_loss: 0.3302\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 1431s 3s/step - loss: 0.1643 - val_loss: 0.3327\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 1431s 3s/step - loss: 0.1620 - val_loss: 0.3334\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 1432s 3s/step - loss: 0.1583 - val_loss: 0.3396\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 1431s 3s/step - loss: 0.1557 - val_loss: 0.3309\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 1431s 3s/step - loss: 0.1528 - val_loss: 0.3391\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 1429s 3s/step - loss: 0.1505 - val_loss: 0.3348\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 1427s 3s/step - loss: 0.1488 - val_loss: 0.3377\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 1430s 3s/step - loss: 0.1463 - val_loss: 0.3341\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 1432s 3s/step - loss: 0.1449 - val_loss: 0.3363\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen,\n",
    " steps_per_epoch=500,\n",
    " epochs=30,\n",
    " validation_data=val_gen,\n",
    " validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Вывод графиков функции потерь и точности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.2.18.png\"></center>\n",
    "<center>Рис. 15. Потери на этапах обучения и проверки в задаче прогнозирования температуры по данным Jena</center>\n",
    "\n",
    "После 3 эпохи наблюдается постепенное расхождение между значениями функции потерь для обучающего и валидационного наборов. Это указывает на переобучение модели. В данном случае для предотвращения переобучения можно прекратить обучение после 3 эпохи.\n",
    "\n",
    "Скомпилируем заново модель, ограничимся 3 эпохами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "500/500 [==============================] - 1035s 2s/step - loss: 0.3136 - val_loss: 0.2699\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 1073s 2s/step - loss: 0.2929 - val_loss: 0.2834\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 877s 2s/step - loss: 0.2829 - val_loss: 0.2762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x274c0b6c760>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(64, return_sequences=True, recurrent_dropout=0.2, dropout=0.2), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Bidirectional(layers.GRU(32, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "model.fit(train_gen,\n",
    " steps_per_epoch=500,\n",
    " epochs=5,\n",
    " validation_data=val_gen,\n",
    " validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    9. Использование обученной сети для предсказания на новых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 10s 99ms/step - loss: 0.2603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2602558732032776"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_gen, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговое значение функции потерь на тестовых данных составляет 0.26.\n",
    "\n",
    "    10. Сопоставление полученных результатов с сетью, первый блок которого является одномерной сверточной сетью (три сверточных слоя), а затем идет второй блок, указанный в варианте:\n",
    "\n",
    "С помощью функции _Sequential()_ создаём новую модель. Первый слой Conv1D с 32 фильтрами размера 7 и функцией активации _relu_, получает на вход временные последовательности данных с любым количеством временных шагов _(None)_ и размерностью признаков, равной размерности исходных данных _(values.shape[-1])_. Этот слой извлекает локальные признаки из входных векторов. Затем добавлен MaxPooling1D(5): слой максимального объединения, который уменьшает размерность выходных данных сверточного слоя путем выбора максимального значения из окна размера 5. Следующий слой анологичен со вторым слоем, который извлекает более высокоуровневые признаки из текстовых данных. После свёрточных слоев добавим слои, которые были в предыдущей модели: добавим слой _Bidirectional_, слой встраивания, который в своем первом аргументе принимает экземпляр рекуррентного слоя _GRU_ с 64 нейронами, параметры _dropout_ и _recurrent_dropout_ применяют регуляризацию отсечением для предотвращения переобучения, параметр _return_sequences_ указывает, что слой должен возвращать полную последовательность выходов, а не только выход на последнем временном шаге. Слой Bidirectional создает второй, отдельный экземпляр этого рекуррентного слоя и использует один экземпляр для обработки входных последовательностей в прямом порядке, а другой — в обратном. Следующий слой _Bidirectional_ такой же как и предыдущий, только рекуррентный слой GRU c 32 нейронами, он не возвращает полную последовательность выходов, а только выход на последнем временном шаге. Последний слой идет полносвязный слой (Dense) с одним нейроном для выполнения задачи регрессии (предсказания одного целевого значения).\n",
    "\n",
    "Скомпелировали модель с помощью функции _compile_. Модель компилируется с той же функции потери mae и оптимизатором RMSprop.\n",
    "\n",
    "Обучение модели производилось в течение 60 эпох, с шагом валидации в 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "500/500 [==============================] - 63s 111ms/step - loss: 0.3481 - val_loss: 0.3097\n",
      "Epoch 2/60\n",
      "500/500 [==============================] - 53s 107ms/step - loss: 0.2996 - val_loss: 0.3324\n",
      "Epoch 3/60\n",
      "500/500 [==============================] - 54s 109ms/step - loss: 0.2782 - val_loss: 0.3300\n",
      "Epoch 4/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.2615 - val_loss: 0.3360\n",
      "Epoch 5/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.2469 - val_loss: 0.3391\n",
      "Epoch 6/60\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.2335 - val_loss: 0.3476\n",
      "Epoch 7/60\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2226 - val_loss: 0.3518\n",
      "Epoch 8/60\n",
      "500/500 [==============================] - 75s 151ms/step - loss: 0.2098 - val_loss: 0.3737\n",
      "Epoch 9/60\n",
      "500/500 [==============================] - 74s 149ms/step - loss: 0.2030 - val_loss: 0.3648\n",
      "Epoch 10/60\n",
      "500/500 [==============================] - 75s 150ms/step - loss: 0.1953 - val_loss: 0.3639\n",
      "Epoch 11/60\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 0.1893 - val_loss: 0.3636\n",
      "Epoch 12/60\n",
      "500/500 [==============================] - 71s 141ms/step - loss: 0.1832 - val_loss: 0.3563\n",
      "Epoch 13/60\n",
      "500/500 [==============================] - 76s 151ms/step - loss: 0.1782 - val_loss: 0.3647\n",
      "Epoch 14/60\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.1744 - val_loss: 0.3771\n",
      "Epoch 15/60\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.1701 - val_loss: 0.3619\n",
      "Epoch 16/60\n",
      "500/500 [==============================] - 75s 151ms/step - loss: 0.1657 - val_loss: 0.3692\n",
      "Epoch 17/60\n",
      "500/500 [==============================] - 73s 147ms/step - loss: 0.1632 - val_loss: 0.3609\n",
      "Epoch 18/60\n",
      "500/500 [==============================] - 75s 151ms/step - loss: 0.1602 - val_loss: 0.3662\n",
      "Epoch 19/60\n",
      "500/500 [==============================] - 57s 114ms/step - loss: 0.1580 - val_loss: 0.3672\n",
      "Epoch 20/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1549 - val_loss: 0.3721\n",
      "Epoch 21/60\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.1530 - val_loss: 0.3652\n",
      "Epoch 22/60\n",
      "500/500 [==============================] - 53s 107ms/step - loss: 0.1506 - val_loss: 0.3649\n",
      "Epoch 23/60\n",
      "500/500 [==============================] - 55s 111ms/step - loss: 0.1481 - val_loss: 0.3670\n",
      "Epoch 24/60\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.1465 - val_loss: 0.3729\n",
      "Epoch 25/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1445 - val_loss: 0.3637\n",
      "Epoch 26/60\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.1424 - val_loss: 0.3674\n",
      "Epoch 27/60\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.1410 - val_loss: 0.3711\n",
      "Epoch 28/60\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.1397 - val_loss: 0.3700\n",
      "Epoch 29/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1370 - val_loss: 0.3595\n",
      "Epoch 30/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1374 - val_loss: 0.3657\n",
      "Epoch 31/60\n",
      "500/500 [==============================] - 54s 109ms/step - loss: 0.1359 - val_loss: 0.3669\n",
      "Epoch 32/60\n",
      "500/500 [==============================] - 50s 101ms/step - loss: 0.1349 - val_loss: 0.3684\n",
      "Epoch 33/60\n",
      "500/500 [==============================] - 50s 101ms/step - loss: 0.1336 - val_loss: 0.3641\n",
      "Epoch 34/60\n",
      "500/500 [==============================] - 51s 101ms/step - loss: 0.1321 - val_loss: 0.3663\n",
      "Epoch 35/60\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.1314 - val_loss: 0.3639\n",
      "Epoch 36/60\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.1304 - val_loss: 0.3578\n",
      "Epoch 37/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1291 - val_loss: 0.3522\n",
      "Epoch 38/60\n",
      "500/500 [==============================] - 53s 105ms/step - loss: 0.1281 - val_loss: 0.3725\n",
      "Epoch 39/60\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.1273 - val_loss: 0.3737\n",
      "Epoch 40/60\n",
      "500/500 [==============================] - 54s 107ms/step - loss: 0.1269 - val_loss: 0.3657\n",
      "Epoch 41/60\n",
      "500/500 [==============================] - 54s 109ms/step - loss: 0.1260 - val_loss: 0.3620\n",
      "Epoch 42/60\n",
      "500/500 [==============================] - 54s 109ms/step - loss: 0.1252 - val_loss: 0.3667\n",
      "Epoch 43/60\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.1242 - val_loss: 0.3676\n",
      "Epoch 44/60\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.1239 - val_loss: 0.3657\n",
      "Epoch 45/60\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.1228 - val_loss: 0.3666\n",
      "Epoch 46/60\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.1226 - val_loss: 0.3681\n",
      "Epoch 47/60\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.1216 - val_loss: 0.3743\n",
      "Epoch 48/60\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.1211 - val_loss: 0.3608\n",
      "Epoch 49/60\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.1203 - val_loss: 0.3713\n",
      "Epoch 50/60\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.1196 - val_loss: 0.3644\n",
      "Epoch 51/60\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.1199 - val_loss: 0.3624\n",
      "Epoch 52/60\n",
      "500/500 [==============================] - 52s 103ms/step - loss: 0.1177 - val_loss: 0.3563\n",
      "Epoch 53/60\n",
      "500/500 [==============================] - 52s 103ms/step - loss: 0.1183 - val_loss: 0.3686\n",
      "Epoch 54/60\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.1180 - val_loss: 0.3593\n",
      "Epoch 55/60\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.1172 - val_loss: 0.3660\n",
      "Epoch 56/60\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.1162 - val_loss: 0.3684\n",
      "Epoch 57/60\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.1169 - val_loss: 0.3679\n",
      "Epoch 58/60\n",
      "500/500 [==============================] - 51s 101ms/step - loss: 0.1164 - val_loss: 0.3705\n",
      "Epoch 59/60\n",
      "500/500 [==============================] - 50s 101ms/step - loss: 0.1158 - val_loss: 0.3685\n",
      "Epoch 60/60\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.1152 - val_loss: 0.3612\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 7, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.Bidirectional( layers.GRU(64, return_sequences=True, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Bidirectional( layers.GRU(32, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    " steps_per_epoch=500,\n",
    " epochs=60,\n",
    " validation_data=val_gen,\n",
    " validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графика функций потерь в течение обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"3.2.19.png\"></center>\n",
    "<center>Рис. 16. Потери на этапах обучения и проверки в задаче прогнозирования температуры по данным Jena</center>\n",
    "\n",
    "На первой эпохе значение функции потерь для обучающего набора составляет 0.3481, а для валидационного набора 0.3097. По мере обучения значения функции потерь для обучающего набора постепенно уменьшаются, что указывает на улучшение модели на обучающих данных, однако значения функции потерь для валидационного набора не демонстрируют снижения. Поскольку значения функции потерь для валидационного набора не снижаются, а увеличиваются- это указывает на переобучение модели. \n",
    "\n",
    "Скомпилируем заново модель, ограничимся 1 эпохой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 61s 109ms/step - loss: 0.3496 - val_loss: 0.3375\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.3563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3562968075275421"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 7, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.Bidirectional( layers.GRU(64, return_sequences=True, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Bidirectional( layers.GRU(32, recurrent_dropout=0.2, dropout=0.2)))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(train_gen,\n",
    " steps_per_epoch=500,\n",
    " epochs=1,\n",
    " validation_data=val_gen,\n",
    " validation_steps=val_steps)\n",
    "model.evaluate(test_gen, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге, получили 2 модели показывающие результаты: первая показывает потери в 0.26, а вторая потери в 0.356. \n",
    "\n",
    "В сравнении с первой моделью значение функции потерь выше, что указывает на избыточно сложную модель для приведенной задачи, а также показывает слабость выделения признаков из больших временных рядов простыми сверточными моделями. \n",
    "\n",
    "<p style=\"text-align: center;\">Заключение</p>\n",
    "\n",
    "1. На первом этапе мы классифицировали отзывы к фильмам на положительные и отрицательные опираясь на эмоциональные окраски отзывов. Использовали набор данных IMDB с 50000 отзывами. Подготовили данные для передачи в сеть с помощью функции _pad_sequences_, т.е. преобразовали списки целых чисел в двумерный тензор с целыми числами и с формой (образцы, максимальная_длина). Сконструировали 2 сети согласно варианту. Превая сеть RNN состоит из первого слоя, _Embedding(num_words, 32, input_length=maxlen)_, слой встраивания. Второго слоя _GRU_, рекуррентный слой c 128 нейронами. Последний слой состоит из одного нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Вторая одномерные сверточная нейронная сеть состоит из первого слоя, _Embedding(num_words, 32, input_length=maxlen)_, слой встраивания, аналогичен с предыдущей модели. Второго слоя Conv1D с 32 фильтрами размера 7 и функцией активации _relu_. Затем добавлен MaxPooling1D(5). Следующий слой анологичен со вторым слоем, который извлекает более высокоуровневые признаки из текстовых данных. Предпоследний слой _GlobalMaxPooling1D()_, слой глобального максимального объединения. Последний слой состоит из одного нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Он выдает вероятность принадлежности входного текста к положительному классу. В итоге, получили и сравнили 2 модели показывающие результаты: первая 85% при потерях в 0.34, вторая 86% при потерях в 1. Как видно точность прогнозирования почти индентичны, но потери одномерной сверточной сети значительно больше.\n",
    "\n",
    "2. На втором этапе мы сделали прогноз температуры по набору данных Jena Climate. В этот набор данных были включены замеры 14 разных характеристик. Изучили принцип работы с временными рядами. Обработали данные с помощью нормализации. Познакомились с  двунаправленной рекуррентной нейронной сетью. Сконструировали 2 сети согласно варианту. Превая сеть RNN состоит из первого слоя, _Bidirectional_, слой встраивания, который в своем первом аргументе принимает экземпляр рекуррентного слоя _GRU_ с 64 нейронами. Второй слой _Bidirectional_ такой же как и первый, только рекуррентный слой GRU c 32 нейронами. Последний слой полносвязный слой Dense с одним нейроном для выполнения задачи регрессии (предсказания одного целевого значения). Вторая сеть первый блок которого является одномерной сверточной сетью (три сверточных слоя), а затем идет второй блок, указанный в варианте. Первый слой Conv1D с 32 фильтрами размера 7 и функцией активации _relu_, получает на вход временные последовательности данных с любым количеством временных шагов _(None)_ и размерностью признаков, равной размерности исходных данных _(values.shape[-1])_. Затем добавлен MaxPooling1D(5): слой максимального объединения. Следующий слой анологичен с первым слоем. После свёрточных слоев добавим слои, которые были в 1 модели: Последний слой идет полносвязный слой (Dense) с одним нейроном для выполнения задачи регрессии (предсказания одного целевого значения). В итоге, получили 2 модели показывающие результаты: первая показывает потери в 0.26, а вторая потери в 0.356. В сравнении с первой моделью значение функции потерь выше, что указывает на избыточно сложную модель для приведенной задачи, а также показывает слабость выделения признаков из больших временных рядов простыми сверточными моделями. \n",
    "\n",
    "<p style=\"text-align: center;\">Список использованной литературы</p>\n",
    "\n",
    "1. Шолле Франсуа. Глубокое обучение на Python. - СПб.: Питер, 2018. - 400 с.: ил. - (Серия «Библиотека программиста»).\n",
    "2. Николенко С., Кадурин А., Архангельская Е. Глубокое обучение. — СПб.: Питер, 2018. — 480 с.: ил. — (Серия «Библиотека программиста»)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
