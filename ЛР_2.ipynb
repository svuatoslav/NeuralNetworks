{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j9I9yolLlWQ"
      },
      "source": [
        "# **Основные этапы работы:**\n",
        "## Этап 0. Установка и настройка оболочки для работы с языком Python\n",
        "Загрузка необходимых библиотек для выполнения лаборатрных работ:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cALQM5NWLlWU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "IMG_HEIGHT = IMG_WIDTH = 150\n",
        "import os, shutil\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "IMG_HEIGHT = IMG_WIDTH = 150\n",
        "\n",
        "#! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aJGuGQoLlWW"
      },
      "source": [
        "## Этап 1. Построение сверточной нейронной сети для распознавания объектов из базы данных MNIST.\n",
        "\n",
        "**Целью данного этапа** лабораторной работы является создание сверточного многокатегориального классификатора изображений цифр из набора данных MNIST.\n",
        "\n",
        "Вариант этапа 1\n",
        "| № | Количество слоев | Количество карт признаков на слое | Размер ядра свертки | Количество скрытых слоев классификатора (количество нейронов на слое)|\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| 3 | 2 | 64-64 | 3x3 | 1 (512) |\n",
        "\n",
        " Пошаговая реализация поставленной цели включает:\n",
        "    1. Загрузка набора данных MNIST:\n",
        "Набор данных MNIST — большой (порядка 60 000 тренировочных и 10 000 проверочных объектов, помеченных на принадлежность одному из десяти классов — какая цифра изображена на картинке) набор картинок с рукописными цифрами, часто используемый для тестирования различных алгоритмов распознавания образов. Он содержит черно-белые картинки размера 28x28 пикселей, исходно взятые из набора образцов из бюро переписи населения США, к которым были добавлены тестовые образцы, написанные студентами американских университетов. \n",
        "\n",
        "    2. Разделение данных на обучающий и тестовый наборы\n",
        "Загрузка данных осуществляется из библиотеки Keras. Данные изначально разделены на тренировочные и тестовые в соотношении 6:1. Размер обучающего набора составляет 60000 экземпляров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jFJz9c6HLlWX"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels),(test_images, test_labels)=mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbnLU3dvmsHc"
      },
      "source": [
        "<center><img src=\"2.1.1.png\"></center>\n",
        "<center>Рис. 1. Образцы изображений из базы MNIST</center>\n",
        "\n",
        "Представленные данные из базы MNIST — изображения — хранятся в трехмерном массиве (60000, 28, 28) типа uint8, значениями в котором являются числа в интервале [0, 255]. Прежде чем использовать их для обучения нейронной сети, мы сначала выполним предварительную обработку данных, нормализуем эти векторы и преобразуем метки классов к векторному представлению. Преобразуем их в форму (60000, 28 х 28) типа float32, которую ожидает получить нейронная сеть, и масштабируем их так, чтобы все значения оказались в интервале [0, 1].\n",
        "\n",
        "    3. Подготовка данных для передачи в нейронную сеть"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NjAFG4fuLlWX"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000,28,28,1))\n",
        "train_images=train_images.astype('float32')/255\n",
        "\n",
        "test_images = test_images.reshape((10000,28,28,1))\n",
        "test_images=test_images.astype('float32')/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для меток используем прямое кодирование (one-hot encoding). Прямое кодирование широко используется для форматирования категорий и также называется кодированием категорий (categorical encoding):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "print(train_images.shape)\n",
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62Y7-W_m0cL"
      },
      "source": [
        "    4. Выполнить конструирование сети: создание сверточной основы в соответствии с вариантом, добавить к сверточной основе классификатор заданной архитектуры, вид активационной функции на сверточном слое и в скрытых слоях классификатора выбрать relu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z9pwXtPKLlWY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Важно отметить, что данная сеть принимает на входе тензоры с формой (высота_ изображения, ширина_изображения, каналы) (не включая измерение, определяющее пакеты). В данном случае мы настроили сеть на обработку входов с размерами (28, 28, 1), соответствующими формату изображений в наборе MNIST, передав аргумент input_shape=(28, 28, 1) в первый слой.\n",
        "\n",
        "С помощью функции _Sequential()_ создаём новую модель. Модель будет состоять из разных видов слоев, сверточных, субдискретизирующих и полносвязных.\n",
        "\n",
        "<center><img src=\"n-cnn.webp\"></center>\n",
        "<center>Рис. 2. Визуализация архитектуры CNN (сверточной нейронной сети)</center>\n",
        "\n",
        "Сверточная сеть состоит из слоев Conv2D и MaxPooling2D. Все слои, Conv2D и MaxPooling2D, выводят трехмерный тензор с формой (высота, ширина, каналы). Измерения ширины и высоты сжимаются с ростом глубины сети. Количество каналов управляется первым аргументом, передаваемым в слои Conv2D (64).\n",
        "\n",
        "Основное отличие полносвязного слоя от сверточного заключается в следующем: слои Dense изучают глобальные шаблоны в пространстве входных признаков (например, в случае с цифрами из набора MNIST это шаблоны, вовлекающие все пиксели), тогда как сверточные слои изучают локальные шаблоны: в случае с изображениями — шаблоны в небольших двумерных окнах во входных данных. Эта выходная карта признаков также является \n",
        "\n",
        "У светрочных нейронных сетей есть 2 важных свойства:\n",
        "1. Шаблоны, которые они изучают, являются инвариантными в отношении переноса.\n",
        "2. Они могут изучать пространственные иерархии шаблонов.\n",
        "\n",
        "Свертка применяется к трехмерным тензорам, называемым картами признаков, с двумя пространственными осями (высота и ширина), а также с осью глубины, для черно-белых изображений она имеет размерность 1. Операция свертывания извлекает шаблоны из своей входной карты признаков и применяет одинаковые преобразования ко всем шаблонам,\n",
        "производя выходную карту признаков трехмерным тензором.\n",
        "\n",
        "Свертки определяются двумя ключевыми параметрами:\n",
        "1. Размер шаблонов, извлекаемых из входных данных, согласно варианту (3 × 3).\n",
        "2. Глубина выходной карты признаков.\n",
        "\n",
        "<center><img src=\"2.1.3.png\"></center>\n",
        "<center>Рис. 3. Схема одного слоя сверточной сети: свертка, за которой следует субдискретизация </center>\n",
        "\n",
        "Полный процесс изображен на рис. 4.\n",
        "\n",
        "<center><img src=\"2.1.4.png\"></center>\n",
        "<center>Рис. 4. Принцип действия свертки</center>\n",
        "\n",
        "Операция MaxPooling2D (выбоа максимального значения из соседних) уменьшает разрешения карты признаков. Операция заключаетя в следующим: из входной карты признаков извлекается окно, и из него выбирается максимальное значение для каждого канала. Это делается для уменьшения количества коэффициентов в карте признаков для обработки, а также внедрения иерархий пространственных фильтров путем создания последовательных слоев свертки для просмотра все более крупных окон.\n",
        "\n",
        "Далее добавляем классификатор поверх сверточной нейронной сети. Сеть передает последний выходной тензор с формой (11, 11, 64) на вход полносвязной классифицирующей сети, стека слоев Dense. Эти классификаторы обрабатывают векторы — одномерные массивы, тогда как текущий выход является трехмерным тензором. Выходы (11, 11, 64) преобразуются в векторы с формой (7744,) перед передачей двум полносвязным слоям Dense. Первый слой состоит из 512 нейронов, с функцией активацией _relu_. Последний слой состоит из 10 нейронов с функцией активации _softmax_, возвращающий массив с 10 оценками вероятностей (в сумме дающих 1). Каждая оценка определяет вероятность принадлежности текущего изображения к одному из 10 классов цифр.\n",
        "\n",
        "Архитектура сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYExIru2LlWY",
        "outputId": "57a12760-2018-4dad-f6dd-8915e9fde654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 7744)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3965440   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4008138 (15.29 MB)\n",
            "Trainable params: 4008138 (15.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2qlofKEnICG"
      },
      "source": [
        "    5. Настройка оптимизатора с выбором функции потерь и метрики качества. Число эпох принять от 30 до 50:\n",
        "\n",
        "Настроить три параметра для этапа компиляции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Функцию потерь определяет, как сеть должна оценивать качество своей работы на обучающих данных. Используем функцию потерь _categorical_crossentropy_. Она определяет расстояние между распределениями вероятностей. В данном случае между распределением вероятности на выходе сети и истинным распределением меток. Минимизируя расстояние между этими двумя распределениями, мы учим сеть выводить результат, максимально близкий к истинным меткам.\n",
        "\n",
        "Оптимизатор — механизм, с помощью которого сеть будет обновлять себя, опираясь на наблюдаемые данные и функцию потерь. Настраиваем модель оптимизатором _rmsprop_ и функцией потерь _mеап squared error_.\n",
        "\n",
        "Среднеквадратичное распространение корня (_rmsprop_) - это экспоненциально затухающее среднее значение. Существенным свойством _rmsprop_ является то, что вы не ограничены только суммой прошлых градиентов, но вы более ограничены градиентами последних временных шагов. В _rmsprop_ мы пытаемся уменьшить вертикальное движение, используя среднее значение, потому что они суммируются приблизительно до 0, принимая среднее значение. _rmsprop_ предоставляет среднее значение для обновления. Формула обновления изображена на рисунке 5\n",
        "\n",
        "<center><img src=\"2.1.8.png\"></center>\n",
        "<center>Рис. 5. Формула rmsprop</center>\n",
        "\n",
        "Метрика для мониторинга на этапах обучения и тестирования выберем точность (доля правильно классифицированных изображений). _Accuracy_ (точность) — это показатель, который описывает общую точность предсказания модели по всем классам.\n",
        "\n",
        "    6. Проведение проверки решения, выделяя контрольное множество:\n",
        "\n",
        "Для контроля точности выделим проверочное множество 10000 образцов из обучающего множества данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vTAKB3SALlWZ"
      },
      "outputs": [],
      "source": [
        "val_images = train_images[:10000]\n",
        "partial_train_images = train_images[10000:]\n",
        "val_labels = train_labels[:10000]\n",
        "partial_train_labels = train_labels[10000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучим модель 50 эпохах пакетами (batch_size) по 64 образцов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy8uI6HRLlWZ",
        "outputId": "65cbc5fe-05cf-4bdd-fabd-d4884c85026b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\svyatoslav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.1319 - accuracy: 0.9588 - val_loss: 0.0640 - val_accuracy: 0.9815\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.0495 - val_accuracy: 0.9861\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 22s 29ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0589 - val_accuracy: 0.9857\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.0684 - val_accuracy: 0.9848\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0561 - val_accuracy: 0.9879\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0505 - val_accuracy: 0.9897\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0581 - val_accuracy: 0.9896\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0641 - val_accuracy: 0.9899\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0634 - val_accuracy: 0.9907\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0894 - val_accuracy: 0.9887\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0817 - val_accuracy: 0.9892\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0850 - val_accuracy: 0.9902\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 3.1509e-04 - accuracy: 0.9999 - val_loss: 0.1027 - val_accuracy: 0.9883\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.0920 - val_accuracy: 0.9896\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 9.2459e-04 - accuracy: 0.9997 - val_loss: 0.0935 - val_accuracy: 0.9900\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0840 - val_accuracy: 0.9905\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 1.4319e-04 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9910\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 2.8343e-06 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9909\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 1.4318e-06 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 0.9908\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 1.1210e-06 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9908\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 9.4382e-07 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9907\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 26s 34ms/step - loss: 8.2928e-07 - accuracy: 1.0000 - val_loss: 0.0877 - val_accuracy: 0.9907\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 7.3087e-07 - accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9906\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 6.5554e-07 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9906\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 6.0590e-07 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9906\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 5.5990e-07 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9906\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 5.1595e-07 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9906\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 4.8179e-07 - accuracy: 1.0000 - val_loss: 0.0911 - val_accuracy: 0.9907\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 4.5354e-07 - accuracy: 1.0000 - val_loss: 0.0914 - val_accuracy: 0.9906\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 4.2630e-07 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 0.9905\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 4.0379e-07 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9905\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 3.7854e-07 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 0.9907\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 3.6445e-07 - accuracy: 1.0000 - val_loss: 0.0927 - val_accuracy: 0.9905\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 3.4690e-07 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 0.9905\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 3.3287e-07 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9905\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 3.1886e-07 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 0.9905\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 3.0466e-07 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9905\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.9377e-07 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9905\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 2.8346e-07 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9905\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.7281e-07 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9905\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.6443e-07 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9905\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.5577e-07 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9905\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 2.4717e-07 - accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9905\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 2.4017e-07 - accuracy: 1.0000 - val_loss: 0.0955 - val_accuracy: 0.9905\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.3238e-07 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9905\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 2.2636e-07 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9905\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 2.2041e-07 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9905\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 2.1452e-07 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9905\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 2.0859e-07 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9907\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 2.0928e-07 - accuracy: 1.0000 - val_loss: 0.0966 - val_accuracy: 0.9906\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(partial_train_images, partial_train_labels, epochs=50, batch_size=64, validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIKOPI9nVg5"
      },
      "source": [
        "    7. Вывод графиков функции потерь и точности:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "DznW0T03LlWa",
        "outputId": "174d9e85-410d-4f24-e419-f5ec8bea2af9"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)#accuracy loss_values\n",
        "\n",
        "plt.plot(epochs, loss_values, 'r', label='Training loss')#ro\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"2.1.5.png\"></center>\n",
        "<center>Рис. 6. Потери на этапах обучения и проверки</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "whe87a7dLlWa",
        "outputId": "0fbdabad-c5b1-4f0f-ee4e-b1427b7ea421"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'ro', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN_vTwmbnZqi"
      },
      "source": [
        "<center><img src=\"2.1.6.png\"></center>\n",
        "<center>Рис. 7. Точность на этапах обучения и проверки</center>\n",
        "\n",
        "\n",
        "Лучшее значение метрики функции потерь и точности на валидационном множестве в процессе обучения составляет 0.08 и 0.99 соответственно. Как можно видеть по графикам переобучение наступает после 17 эпохе.\n",
        "\n",
        "    8. Использование обученной сети для предсказания на новых данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRfT7c7pLlWa",
        "outputId": "f2317978-72f1-4120-e62b-663c1d56360d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 5ms/step - loss: 0.0577 - accuracy: 0.9925\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Модель показала точность в 99%, с потерями 0.06. Отличная точность. С подобными данными модель хорошо определяет цифры в виде похожих изображений.\n",
        "\n",
        "Пример работы обученной сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = model.predict(test_images)\n",
        "fig = plt.figure\n",
        "plt.imshow(test_images[0], cmap='gray')\n",
        "plt.show()\n",
        "print(\"Predicted:\", np.argmax(prediction[0]), \"Target:\", np.argmax(test_labels[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"2.1.7.png\"></center>\n",
        "<center>Рис. 8. Визуализация предсказания обученной модели.</center>\n",
        "\n",
        "_Predicted: 7 Target: 7_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLcK4M5FLlWa"
      },
      "source": [
        "## Этап 2. Построение бинарного классификатора. Обучение на малом объеме данных\n",
        "\n",
        "**Целью данного этапа** лабораторной работы является создание бинарного классификатора изображений кошек и собак из набора данных Cats vs. Dogs.\n",
        "\n",
        "Вариант этапа 2\n",
        "| № | Размер входного изображения | Количество слоев | Количество карт признаков на слое | Размер ядра свертки | Количество скрытых слоев классификатора (количество нейронов на слое) |\n",
        "| :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| 3 | 150x150 | 3 | 64-128-128 | 3x3 | 1 (1024) |\n",
        "\n",
        " Пошаговая реализация поставленной цели включает:\n",
        "\n",
        "    1. Загрузка набора данных Cats vs. Dogs:\n",
        "Набор данных Cats vs. Dogs представляет собой 25000 изображений различного размера. По 12500 изображений выделено для каждого класса (кошек и собак).\n",
        "\n",
        "    2. Разделение данных на обучающий и тестовый наборы:\n",
        "Загрузка данных осуществляется по ссылке из Kaggle. Данные изначально не разделены."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pqf5qDAnjjC"
      },
      "source": [
        "Загрузка данных осуществляется при помощи подключения к Google Drive, где хранится архив с данными. Затем этот архив распаковывается в рабочую директорию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "M2RTkgkxLwIf",
        "outputId": "a7fb2fdf-4209-49c4-9bad-8f0a5de8ff95"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "! kaggle datasets download -d \"bhavikjikadara/dog-and-cat-classification-dataset\"\n",
        "! mkdir datasets\n",
        "! unzip dog-and-cat-classification-dataset.zip -d datasets\n",
        "# local_zip = '/content/dog-and-cat-classification-dataset.zip'\n",
        "# zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall('/')\n",
        "# zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создадим базовый каталог для сохранения выделенного небольшого набора: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS0ofO6ALz0L"
      },
      "outputs": [],
      "source": [
        "base_dir = '/CatsDogs'\n",
        "os.mkdir(base_dir)\n",
        "#train_dir = os.path.join(base_dir, 'train')\n",
        "#validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "#train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "#train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "#validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "#validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запомним пути в каталоги, которые в последующим будем создавать и использовать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw6cT-wLM2fB"
      },
      "outputs": [],
      "source": [
        "original_dataset_dir_Cat = \"/content/datasets/PetImages/Cat\"\n",
        "original_dataset_dir_Dog = \"/content/datasets/PetImages/Dog\"\n",
        "original_dataset_dir = \"/content/datasets/PetImages\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qzl6SxLLlWa"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(base_dir,'train')\n",
        "validation_dir  = os.path.join(base_dir,'validation')\n",
        "test_dir  = os.path.join(base_dir,'test')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir,'cats')\n",
        "train_dogs_dir = os.path.join(train_dir,'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir,'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir,'dogs')\n",
        "\n",
        "test_cats_dir = os.path.join(test_dir,'cats')\n",
        "test_dogs_dir = os.path.join(test_dir,'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создадим пустые каталоги."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvGCGI5eNxWF"
      },
      "outputs": [],
      "source": [
        "os.mkdir(train_dir)\n",
        "os.mkdir(validation_dir)\n",
        "os.mkdir(test_dir)\n",
        "os.mkdir(train_cats_dir)\n",
        "os.mkdir(train_dogs_dir)\n",
        "os.mkdir(validation_cats_dir)\n",
        "os.mkdir(validation_dogs_dir)\n",
        "os.mkdir(test_cats_dir)\n",
        "os.mkdir(test_dogs_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Распределим изображения в обучающий, проверочный и контрольный каталоги:\n",
        "\n",
        "Разделим их на 2000 обучающих, 1000 проверочных и 1000 контрольных изображений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYveiFXmLlWb"
      },
      "outputs": [],
      "source": [
        "cats_list = os.listdir(original_dataset_dir_Cat)\n",
        "dogs_list = os.listdir(original_dataset_dir_Dog)\n",
        "\n",
        "fnames = cats_list[:1000]#1000\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames =cats_list[1000:1500] #1000,1500\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =cats_list[1500:2000]#1500,2000\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =dogs_list[:1000]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames =dogs_list[1000:1500]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =dogs_list[1500:2000]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    3. Создание генератора изображений заданной размерности\n",
        "\n",
        "Создадим объект ImageDataGenerator с несколькими параметрами для аугментации данных (rescale, rotation_range, width_shift, height_shift, shear_range, zoom_range, horizontal_flip, fill_mode) и предварительной обработки изображений. ImageDataGenerator поможет модели выявить больше особенностей данных и достичь лучшей степени обобщения, ImageDataGenerator реализует подход создания дополнительных обучающих данных из имеющихся путем трансформации образцов множеством случайных преобразований, дающих правдоподобные изображения.\n",
        "*   rescale — масштабирование значения с коэффициентом 1/255.\n",
        "*   rotation_range — величина в градусах (0–180), диапазон, в котором будет осуществляться случайный поворот изображения;\n",
        "*   width_shift и height_shift — диапазоны (в долях ширины и высоты), в пределах которых изображения смещаются по горизонтали и вертикали соответственно;\n",
        "*   shear_range — для случайного применения сдвигового (shearing) преобразования;\n",
        "*   zoom_range — для случайного изменения масштаба внутри изображений;\n",
        "*   horizontal_flip — для случайного переворачивания половины изображения по горизонтали — подходит в случае отсутствия предположений о горизонтальной асимметрии;\n",
        "*   fill_mode — стратегия заполнения вновь созданных пикселов, появляющихся после поворота или смещения по горизонтали/вертикали.\n",
        "\n",
        "Этот объект будет использоваться для генерации обучающих и валидационных данных.\n",
        "\n",
        "Обучение сверточной нейронной сети с использованием генераторов расширения данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jencygCuLlWb"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    4. Подготовку данных для передачи в нейронную сеть:\n",
        "\n",
        "Получим тренировочный, валидационный, тестовый генераторы данных, а также словарь сопоставления номера класса с текстовой меткой. Так как используется функция потерь binary_crossentropy, метки должны быть бинарными поставим _class_mode='binary'_, согласно, варианту приведём изображения к размеру _150 × 150: target_size=(150, 150)_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j0CvvAcLlWb",
        "outputId": "2c209da2-028c-4642-cbf2-5bd47fab08ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    5. Выполнить конструирование сети: создание сверточной основы в соответствии с вариантом, добавить к сверточной основе классификатор заданной архитектуры, вид активационной функции на сверточном слое и в скрытых слоях классификатора выбрать relu:\n",
        "\n",
        "С помощью функции _Sequential()_ создаём новую модель. Сверточная часть сети состоит из слоев _Conv2D_ и _MaxPooling2D_. На входе поставим размер изображения _150 x 150_. Количество карт признаков на первом слое _Conv2D_ 64, размер ядра свертки 3x3, с функцией активацией _relu_ после каждого слоя _Conv2D_ применяем _MaxPooling2D_, уменьшая размер карты признаков вдвое. На следующих слоях будем применять тот же стек 2 раза, только изменяя количество карт признаков на слоях _Conv2D_ с 64 на 128. Далее добавляем классификатор поверх сверточной нейронной сети. Первый слой состоит из 1024 нейронов, с функцией активацией _relu_. Последний слой состоит из 1 нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGMCLZtoLlWb",
        "outputId": "88d77c4a-e7a6-4e5c-9592-770b936673bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    6. Настройка оптимизатора с выбором функции потерь и метрики качества. Число эпох принять от 30 до 50:\n",
        "\n",
        "Функция потерь перекрестная энтропия (binary_crossentropy) - определяет меру расстояния между распределениями вероятностей, или в данном случае - между фактическими данными и предсказаниями. Настраиваем модель с оптимизатором rmsprop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    7. Проведение проверки решения, выделяя контрольное множество:\n",
        "Обучение модели производилось в течение 50 эпох, данные разбивались на батчи, размером 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUQ27L3TLlWc",
        "outputId": "cc1dca7e-b2cc-4fd2-db91-e68f73b0891d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 22s 175ms/step - loss: 0.7241 - acc: 0.4925 - val_loss: 0.6926 - val_acc: 0.5250\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.6927 - acc: 0.5405 - val_loss: 0.7009 - val_acc: 0.5080\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.6882 - acc: 0.5650 - val_loss: 0.6529 - val_acc: 0.6390\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.6688 - acc: 0.5810 - val_loss: 0.6389 - val_acc: 0.5990\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 19s 190ms/step - loss: 0.6737 - acc: 0.6045 - val_loss: 0.6073 - val_acc: 0.6790\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 19s 185ms/step - loss: 0.6608 - acc: 0.6225 - val_loss: 0.5818 - val_acc: 0.7100\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.6508 - acc: 0.6155 - val_loss: 0.5662 - val_acc: 0.6920\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.6426 - acc: 0.6320 - val_loss: 0.5886 - val_acc: 0.7100\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.6243 - acc: 0.6540 - val_loss: 0.6279 - val_acc: 0.6100\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.6196 - acc: 0.6600 - val_loss: 0.5504 - val_acc: 0.7200\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.6118 - acc: 0.6715 - val_loss: 0.5565 - val_acc: 0.7110\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.6185 - acc: 0.6690 - val_loss: 0.5261 - val_acc: 0.7390\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 18s 177ms/step - loss: 0.5999 - acc: 0.6865 - val_loss: 0.5124 - val_acc: 0.7320\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.6028 - acc: 0.6855 - val_loss: 0.5606 - val_acc: 0.6900\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.6033 - acc: 0.6785 - val_loss: 0.5266 - val_acc: 0.7300\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.5887 - acc: 0.6875 - val_loss: 0.7787 - val_acc: 0.6490\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.5931 - acc: 0.6875 - val_loss: 0.4987 - val_acc: 0.7560\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 18s 181ms/step - loss: 0.5772 - acc: 0.6985 - val_loss: 0.5204 - val_acc: 0.7460\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.5793 - acc: 0.6870 - val_loss: 0.4851 - val_acc: 0.7540\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.5730 - acc: 0.6945 - val_loss: 0.5690 - val_acc: 0.6980\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.5754 - acc: 0.6930 - val_loss: 0.4825 - val_acc: 0.7600\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 19s 192ms/step - loss: 0.5536 - acc: 0.7235 - val_loss: 0.4788 - val_acc: 0.7610\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.5376 - acc: 0.7200 - val_loss: 0.5042 - val_acc: 0.7590\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.5559 - acc: 0.7220 - val_loss: 0.5151 - val_acc: 0.7580\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.5578 - acc: 0.7170 - val_loss: 0.5034 - val_acc: 0.7760\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.5475 - acc: 0.7255 - val_loss: 0.5208 - val_acc: 0.7560\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 18s 181ms/step - loss: 0.5371 - acc: 0.7370 - val_loss: 0.5840 - val_acc: 0.6870\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.5540 - acc: 0.7255 - val_loss: 0.4812 - val_acc: 0.7620\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.5473 - acc: 0.7185 - val_loss: 0.5111 - val_acc: 0.7510\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.5312 - acc: 0.7265 - val_loss: 0.4500 - val_acc: 0.7940\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.5202 - acc: 0.7445 - val_loss: 0.4757 - val_acc: 0.7800\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.5294 - acc: 0.7375 - val_loss: 0.4656 - val_acc: 0.7900\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 19s 193ms/step - loss: 0.5280 - acc: 0.7345 - val_loss: 0.5027 - val_acc: 0.7310\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 18s 180ms/step - loss: 0.5247 - acc: 0.7465 - val_loss: 0.4771 - val_acc: 0.7960\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 19s 184ms/step - loss: 0.5160 - acc: 0.7560 - val_loss: 0.5615 - val_acc: 0.7600\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.5192 - acc: 0.7485 - val_loss: 0.4532 - val_acc: 0.8010\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.5102 - acc: 0.7610 - val_loss: 0.4640 - val_acc: 0.7820\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 18s 180ms/step - loss: 0.5058 - acc: 0.7595 - val_loss: 0.4888 - val_acc: 0.7660\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 18s 178ms/step - loss: 0.5129 - acc: 0.7585 - val_loss: 0.5534 - val_acc: 0.7380\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 18s 181ms/step - loss: 0.5064 - acc: 0.7465 - val_loss: 0.5215 - val_acc: 0.7520\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.5158 - acc: 0.7580 - val_loss: 0.4858 - val_acc: 0.7690\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.5038 - acc: 0.7645 - val_loss: 0.5656 - val_acc: 0.7480\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 19s 185ms/step - loss: 0.4993 - acc: 0.7615 - val_loss: 0.5014 - val_acc: 0.7750\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.5063 - acc: 0.7580 - val_loss: 0.4691 - val_acc: 0.7820\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 19s 190ms/step - loss: 0.5036 - acc: 0.7585 - val_loss: 0.5659 - val_acc: 0.7200\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.4863 - acc: 0.7650 - val_loss: 0.4582 - val_acc: 0.7990\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.4962 - acc: 0.7575 - val_loss: 0.6027 - val_acc: 0.7350\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.4841 - acc: 0.7765 - val_loss: 0.4990 - val_acc: 0.7670\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.4803 - acc: 0.7785 - val_loss: 0.4264 - val_acc: 0.8110\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 19s 185ms/step - loss: 0.4846 - acc: 0.7685 - val_loss: 0.4830 - val_acc: 0.7870\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=50,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    8. Вывод графиков функции потерь и точности:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "n4gNPI5xLlWc",
        "outputId": "f0661468-c440-4185-a4cc-badb30189a55"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"2.2.1.png\"></center>\n",
        "<center>Рис. 9. Точность на этапах обучения и проверки</center>\n",
        "\n",
        "<center><img src=\"2.2.2.png\"></center>\n",
        "<center>Рис. 10. Потери на этапах обучения и проверки</center>\n",
        "\n",
        "    9. Использование обученной сети для предсказания на новых данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbWn_FrPmTfq",
        "outputId": "fa3a74b9-b731-423f-acd8-1bcc510e71f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.4951 - acc: 0.7715 - val_loss: 0.4594 - val_acc: 0.7970\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.4773 - acc: 0.7705 - val_loss: 0.6351 - val_acc: 0.7160\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.4854 - acc: 0.7640 - val_loss: 0.4871 - val_acc: 0.7830\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.4891 - acc: 0.7675 - val_loss: 0.4502 - val_acc: 0.7790\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 33s 328ms/step - loss: 0.4718 - acc: 0.7750 - val_loss: 0.4978 - val_acc: 0.7730\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.4839 - acc: 0.7710 - val_loss: 0.4722 - val_acc: 0.7950\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 18s 176ms/step - loss: 0.5186 - acc: 0.7605 - val_loss: 0.4468 - val_acc: 0.7990\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 19s 191ms/step - loss: 0.4741 - acc: 0.7820 - val_loss: 0.4387 - val_acc: 0.8060\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.4722 - acc: 0.7850 - val_loss: 0.4775 - val_acc: 0.7810\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=9,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkvo-iJkb3o",
        "outputId": "a9e9f118-c6cf-444b-cc4e-3ab5e2d647bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 2s 39ms/step - loss: 0.4906 - acc: 0.7730\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4905804991722107, 0.7730000019073486]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = model.evaluate(test_generator)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFfDyWxWLlWc"
      },
      "source": [
        "Модель демонстрирует результаты на тестовых данных: функция потерь составляет 0.49, а метрика - 0,77. Из-за относительной простоты архитектуры нейронной сети значение метрики несколько снижено. Для повышения качества модели рекомендуется усложнить архитектуру, добавив дополнительные сверточные слои, а также увеличить объем данных для обучения.\n",
        "\n",
        "Пример работы обученной модели:\n",
        "\n",
        "<center><img src=\"2.2.3.png\"></center>\n",
        "<center>Рис. 11. Визуализация предсказания обученной модели.</center>\n",
        "\n",
        "## Этап 3. Использование предварительно обученной сети.\n",
        "\n",
        "**Целью данного этапа** Целью данного этапа лабораторной работы является использование предварительно обученной на наборе Imagenet нейронной сети VGG16 для увеличения точности классификации изображений из набора данных Cats vs. Dogs.\n",
        "\n",
        "Вариант этапа 3\n",
        "| № | Использование обученной сети | Количество эпох обучения |\n",
        "| :-: | :-: | :-: |\n",
        "| 3 | Выделение признаков обучающего набора при помощи сверточной основы с последующим их использованием как входов классификатора | 50 |\n",
        "\n",
        " Пошаговая реализация поставленной цели включает:\n",
        "\n",
        "    1. Загрузку набора данных Cats vs. Dogs и сети VGG16 с весовыми коэффициентами, полученными при обучении на наборе данных Imagenet:\n",
        "\n",
        "VGG16 - это глубокая сверточная модель нейронной сети, используемая для задач классификации изображений. Сеть состоит из 16 слоев искусственных нейронов, каждый из которых работает над постепенной обработкой информации об изображении и повышением точности ее предсказаний. Она характеризуется своей глубиной, состоящей из 16 слоев, включая 13 сверточных слоев и 3 полностью связанных слоя. Вместо большого количества гиперпараметров, VGG16 использует слои свертки с фильтром 3x3 и шагом 1, которые находятся в том же слое заполнения и maxpool с фильтром 2x2 в шаге 2. Он последовательно использует это расположение слоев свертки и максимального пула во всей архитектуре. В итоге у него есть два полностью соединенных слоя, за которыми следует softmax для вывода. В ней около 138 миллионов параметров.\n",
        "\n",
        "<center><img src=\"2.3.1.png\"></center>\n",
        "\n",
        "<center>Рис. 12. Общая структура сети</center>\n",
        "\n",
        "Архитектура:\n",
        "\n",
        "Архитектура VGG-16 представляет собой глубокую сверточную нейронную сеть (CNN), разработанную для задач классификации изображений.\n",
        "\n",
        "Конфигурация VGG-16 обычно состоит из 16 слоев, включая 13 сверточных слоев и 3 полностью связанных слоя. Эти слои организованы в блоки, причем каждый блок содержит несколько сверточных слоев, за которыми следует слой с максимальным объединением для понижающей дискретизации.\n",
        "\n",
        "<center><img src=\"2.3.2.png\"></center>\n",
        "\n",
        "<center>Рис. 13. Карта архитектуры VGG-16</center>\n",
        "\n",
        "Архитектура VGG16 на основе рисунка 13:\n",
        "\n",
        "1. Входной слой:\n",
        "* Входные размеры: (224, 224, 3)\n",
        "2. Сверточные слои (64 фильтра, 3 × 3 фильтра, одинаковое заполнение):\n",
        "* Два последовательных сверточных слоя с 64 фильтрами в каждом и размером фильтра 3 × 3.\n",
        "* Такое же заполнение применяется для сохранения пространственных размеров.\n",
        "3. Максимальный уровень объединения (2 × 2, шаг 2):\n",
        "* Максимальный уровень объединения с размером пула 2 × 2 и шагом 2.\n",
        "4. Сверточные слои (128 фильтров, 3 × 3 фильтра, одинаковое заполнение):\n",
        "* Два последовательных сверточных слоя по 128 фильтров в каждом и размер фильтра 3 × 3.\n",
        "5. Максимальный уровень объединения (2 × 2, шаг 2):\n",
        "* Максимальный уровень объединения с размером пула 2 × 2 и шагом 2.\n",
        "6. Сверточные слои (256 фильтров, 3 × 3 фильтра, одинаковое заполнение):\n",
        "* Два последовательных сверточных слоя с 256 фильтрами в каждом и размером фильтра 3 × 3.\n",
        "7. Сверточные слои (512 фильтров, 3 × 3 фильтра, одинаковое заполнение):\n",
        "* Два набора из трех последовательных сверточных слоев с 512 фильтрами в каждом и размером фильтра 3 × 3.\n",
        "8. Максимальный уровень объединения (2 × 2, шаг 2):\n",
        "* Максимальный уровень объединения с размером пула 2 × 2 и шагом 2.\n",
        "Стек сверточных слоев и максимальный пул:\n",
        "8. Два дополнительных сверточных слоя после предыдущего стека.\n",
        "* Размер фильтра: 3 × 3.\n",
        "9. Сплющивание:\n",
        "* Сведение выходной карты объектов (7x7x512) в вектор размером 25088.\n",
        "10. Полностью подключенные слои:\n",
        "* Три полностью соединенных слоя с активацией ReLU.\n",
        "* Первый слой с размером ввода 25088 и размером вывода 4096.\n",
        "* Второй слой с размером ввода 4096 и размером вывода 4096.\n",
        "* Третий уровень с размером входных данных 4096 и размером выходных данных 1000, соответствующий 1000 классам в ILSVRC challenge.\n",
        "* Активация Softmax применяется к выходу третьего полностью подключенного уровня для классификации.\n",
        "\n",
        "Эта архитектура соответствует предоставленным спецификациям, включая использование функции активации ReLU и конечного полностью подключенного уровня, выводящего вероятности для 1000 классов с использованием активации softmax.\n",
        "\n",
        "Ограничения VGG 16:\n",
        "* Она очень медленно обучается (оригинальная модель VGG обучалась на графическом процессоре Nvidia Titan в течение 2-3 недель).\n",
        "* Размер обученных весов ImageNet в VGG-16 составляет 528 МБ. Таким образом, он занимает довольно много места на диске и пропускной способности, что делает его неэффективным.\n",
        "* 138 миллионов параметров приводят к проблеме резкого увеличения градиентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmsokDoPLlWc",
        "outputId": "bf2a50cf-0aba-439a-eddd-6088be617267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import VGG16\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "  include_top=False,\n",
        "  input_shape=(150, 150, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    2. Разделение данных на обучающий и тестовый наборы:\n",
        "Загрузка данных осуществляется аналогично как у предыдущего этапа:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cats_list = os.listdir(original_dataset_dir_Cat)\n",
        "dogs_list = os.listdir(original_dataset_dir_Dog)\n",
        "\n",
        "fnames = cats_list[:1000]#1000\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames =cats_list[1000:1500] #1000,1500\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =cats_list[1500:2000]#1500,2000\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Cat, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =dogs_list[:1000]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src,dst)\n",
        "\n",
        "fnames =dogs_list[1000:1500]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames =dogs_list[1500:2000]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir_Dog, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    3. Создание генератора изображений заданной размерности:\n",
        "\n",
        "Эффективным подходом к глубокому обучению на небольших наборах изображений является использование предварительно обученной сети. Предварительно обученная сеть — это сохраненная сеть, прежде обученная на большом наборе данных, обычно в рамках масштабной задачи классификации изображений. Если исходный набор данных достаточно велик и достаточно обобщен, тогда пространственная иерархия признаков, изученных сетью, может эффективно выступать в роли обобщенной модели видимого мира и быть полезной во многих разных задачах распознавания образов, даже если эти новые задачи будут связаны с совершенно иными классами, отличными от классов в оригинальной задаче. В нашем случае мы возьмем за основу сверточную нейронную сеть, обученную на наборе ImageNet (1,4 миллиона изображений, классифицированных на 1000 разных классов). Коллекция ImageNet содержит множество изображений разных\n",
        "животных, включая разновидности кошек и собак, а значит, можно рассчитывать, что модель, обученная на этой коллекции, прекрасно справится с нашей задачей классификации изображений кошек и собак\n",
        "\n",
        "Выделение признаков заключается в использовании представлений, изученных предыдущей сетью, для выделения признаков из новых образцов, которые затем пропускаются через новый классификатор, обучаемый с нуля. Процесс выделения\n",
        "признаков заключается в том, чтобы взять сверточную основу предварительно обученной сети, пропустить через нее новые данные и на основе вывода обучить новый классификатор.\n",
        "\n",
        "Получим признаки из изображений путем применения функции _extract_features_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEnkg6YiWfd4",
        "outputId": "ad48bc39-3fbd-4bb1-d250-8279f31525bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Found 1000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Found 1000 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        }
      ],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "def extract_features(directory, sample_count):\n",
        "  features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape=(sample_count))\n",
        "  generator = datagen.flow_from_directory(\n",
        "    directory,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = conv_base.predict(inputs_batch)\n",
        "    features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "    labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= sample_count:\n",
        "      break\n",
        "  return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, 2000)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
        "test_features, test_labels = extract_features(test_dir, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    4. Подготовка данных для передачи в нейронную сеть:\n",
        "Объем тренировочного множества составил 2000 экземпляров, а объем валидационного и тренировочного по 1000 экземпляров. Полученные признаки были приобразованы для подачи в модель нейронной сети методом reshape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1l5-FO9c1Mt"
      },
      "outputs": [],
      "source": [
        "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
        "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
        "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    5. В соответствие с вариантом сконструируйте сеть, используя вариант классификатора (слои расположенные после сверточной основы) из этапа 2:\n",
        "С помощью функции _Sequential()_ создаём новую модель. Первый слой состоит из 1024 нейронов, с функцией активацией _relu_. Последний слой состоит из 1 нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Для преодоления проблемы переобучения, применим метод Dropout.\n",
        "\n",
        "Прореживание (dropout) — один из наиболее эффективных и распространенных приемов регуляризации для нейронных сетей. Прореживание, которое применяется к слою, заключается в удалении (присваивании нуля) случайно выбираемым признакам на этапе обучения. . Коэффициент прореживания — это доля обнуляемых признаков, в данном случае выбрали 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1024, activation='relu', input_dim=4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    6. Настройка оптимизатора с выбором функции потерь и метрики качества, используя количество эпох обучения согласно варианту;\n",
        "    7. Проведение проверки решения, выделяя контрольное множество:\n",
        "Обучение модели производилось в течение 50 эпох, данные разбивались на батчи, размером 20. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbnkXD_OcYo4",
        "outputId": "25ab987e-11e1-4f4e-e663-18272a91bea1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 1.1621 - acc: 0.7355 - val_loss: 1.2101 - val_acc: 0.5850\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.4358 - acc: 0.8220 - val_loss: 0.3276 - val_acc: 0.8580\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.3187 - acc: 0.8730 - val_loss: 0.3372 - val_acc: 0.8660\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.2841 - acc: 0.8910 - val_loss: 0.2649 - val_acc: 0.8960\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.2428 - acc: 0.9065 - val_loss: 0.3407 - val_acc: 0.8720\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.2054 - acc: 0.9225 - val_loss: 0.2906 - val_acc: 0.8970\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1910 - acc: 0.9240 - val_loss: 0.2908 - val_acc: 0.8900\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1755 - acc: 0.9275 - val_loss: 0.3249 - val_acc: 0.8910\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1371 - acc: 0.9450 - val_loss: 0.3275 - val_acc: 0.8880\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1172 - acc: 0.9530 - val_loss: 0.7439 - val_acc: 0.8230\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1359 - acc: 0.9490 - val_loss: 0.4012 - val_acc: 0.8860\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.1092 - acc: 0.9570 - val_loss: 0.4059 - val_acc: 0.8840\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0870 - acc: 0.9680 - val_loss: 0.4312 - val_acc: 0.8990\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0988 - acc: 0.9655 - val_loss: 0.4383 - val_acc: 0.8910\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0750 - acc: 0.9785 - val_loss: 0.4576 - val_acc: 0.8920\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0536 - acc: 0.9820 - val_loss: 0.7899 - val_acc: 0.8570\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0654 - acc: 0.9780 - val_loss: 0.7303 - val_acc: 0.8610\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0659 - acc: 0.9750 - val_loss: 0.5589 - val_acc: 0.8820\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0488 - acc: 0.9820 - val_loss: 0.5697 - val_acc: 0.8850\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0578 - acc: 0.9850 - val_loss: 0.5887 - val_acc: 0.8940\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0655 - acc: 0.9790 - val_loss: 0.6381 - val_acc: 0.8810\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0324 - acc: 0.9885 - val_loss: 0.6989 - val_acc: 0.8790\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0402 - acc: 0.9855 - val_loss: 0.6723 - val_acc: 0.8870\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0405 - acc: 0.9880 - val_loss: 0.6559 - val_acc: 0.8870\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0350 - acc: 0.9895 - val_loss: 0.9041 - val_acc: 0.8750\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0361 - acc: 0.9875 - val_loss: 0.7308 - val_acc: 0.8960\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0407 - acc: 0.9880 - val_loss: 0.7595 - val_acc: 0.8990\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0387 - acc: 0.9900 - val_loss: 0.8011 - val_acc: 0.8830\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.0181 - acc: 0.9955 - val_loss: 0.8591 - val_acc: 0.8850\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0324 - acc: 0.9900 - val_loss: 0.7923 - val_acc: 0.8960\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0253 - acc: 0.9935 - val_loss: 1.4642 - val_acc: 0.8470\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.8733 - val_acc: 0.8840\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.8381 - val_acc: 0.8840\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0353 - acc: 0.9925 - val_loss: 0.8859 - val_acc: 0.8880\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0173 - acc: 0.9930 - val_loss: 0.8154 - val_acc: 0.8930\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0273 - acc: 0.9915 - val_loss: 0.9888 - val_acc: 0.8790\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0305 - acc: 0.9925 - val_loss: 1.0536 - val_acc: 0.8900\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0159 - acc: 0.9930 - val_loss: 0.9931 - val_acc: 0.8940\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0332 - acc: 0.9905 - val_loss: 1.1336 - val_acc: 0.8830\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0298 - acc: 0.9890 - val_loss: 1.0738 - val_acc: 0.8920\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0355 - acc: 0.9905 - val_loss: 0.9899 - val_acc: 0.8830\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.9948 - val_acc: 0.8840\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0366 - acc: 0.9935 - val_loss: 1.0736 - val_acc: 0.8930\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0225 - acc: 0.9920 - val_loss: 0.9646 - val_acc: 0.8890\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0161 - acc: 0.9945 - val_loss: 1.0720 - val_acc: 0.8810\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0132 - acc: 0.9965 - val_loss: 1.0309 - val_acc: 0.8890\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0154 - acc: 0.9950 - val_loss: 1.0867 - val_acc: 0.8900\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0194 - acc: 0.9930 - val_loss: 1.0476 - val_acc: 0.8910\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0260 - acc: 0.9935 - val_loss: 1.1626 - val_acc: 0.8750\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.0244 - acc: 0.9925 - val_loss: 1.0224 - val_acc: 0.8890\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=RMSprop(lr=2e-5),\n",
        " loss='binary_crossentropy',\n",
        " metrics=['acc'])\n",
        "history = model.fit(train_features, train_labels,\n",
        " epochs=50,\n",
        " batch_size=20,\n",
        " validation_data=(validation_features, validation_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    8. Вывод графиков функции потерь и точности:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "8SWsQmtOdXso",
        "outputId": "bc2db7b0-d093-4168-9334-b35c5aa9ae41"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"2.3.3.png\"></center>\n",
        "<center>Рис. 14. Точность на этапах обучения и проверки</center>\n",
        "\n",
        "<center><img src=\"2.3.4.png\"></center>\n",
        "<center>Рис. 15. Потери на этапах обучения и проверки</center>\n",
        "\n",
        "На 9 эпохе происходит переобучение модели. Лучшее значение функции потерь и метрики на валидационном наборе данных составляют 0.3275 и 0.8880 соответственно.\n",
        "\n",
        "    9. Использование обученной сети для предсказания на новых данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdFWbsIeN9a",
        "outputId": "7fa56b3a-bad1-4585-a0bf-83d7fae034d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0073 - acc: 0.9970 - val_loss: 1.0577 - val_acc: 0.8750\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0109 - acc: 0.9970 - val_loss: 1.1070 - val_acc: 0.8880\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0041 - acc: 0.9975 - val_loss: 1.1580 - val_acc: 0.8850\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0186 - acc: 0.9935 - val_loss: 1.1561 - val_acc: 0.8970\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0231 - acc: 0.9920 - val_loss: 1.5697 - val_acc: 0.8820\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0198 - acc: 0.9955 - val_loss: 1.1466 - val_acc: 0.8940\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0195 - acc: 0.9930 - val_loss: 1.2472 - val_acc: 0.8810\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.2568 - val_acc: 0.8800\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 8.8506e-04 - acc: 0.9995 - val_loss: 1.2809 - val_acc: 0.8810\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79fb30685bd0>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_features, train_labels,\n",
        " epochs=9,\n",
        " batch_size=20,\n",
        " validation_data=(validation_features, validation_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfRp5SyKd4hT",
        "outputId": "66dffdf8-af6d-4e9d-bef5-4c1daaf49026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 3ms/step - loss: 1.1114 - acc: 0.9010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.1113535165786743, 0.9010000228881836]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = model.evaluate(test_features,test_labels)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Итоговое значение функции потерь и метрики на тестовых данных составляет 1.11 и 0.9 соответственно. Для улучшения качества работы модели можно использовать больший объем данных для обучения.\n",
        "\n",
        "Пример работы обученной модели:\n",
        "\n",
        "<center><img src=\"2.3.5.png\"></center>\n",
        "<center>Рис. 16. Визуализация предсказания обученной модели</center>\n",
        "\n",
        "<p style=\"text-align: center;\">Заключение</p>\n",
        "\n",
        "1. На первом этапе мы классифицировали изображения цифр из набора данных MNIST. Подготовили данные для передачи в сеть нормализовали входные изображения, и преобразовали метки классов к векторному представлению. Сконструировали сверточную нейронную сеть (CNN).  Сверточная часть сети состоит из слоев _Conv2D_ и _MaxPooling2D_. На входе размер изображения _150 x 150_. Количество карт признаков на первом слое _Conv2D_ 32, размер ядра свертки 3x3, с функцией активацией _relu_ после каждого слоя _Conv2D_ применяем _MaxPooling2D_, уменьшая размер карты признаков вдвое. На следующем слое применили тот же стек, только изменяя количество карт признаков на слоях _Conv2D_ с 32 на 64. Добавили классификатор поверх сверточной нейронной сети. Первый слой состоит из 512 нейронов, с функцией активацией _relu_. Последний слой состоит из 10 нейронов с функцией активации _softmax_, возвращающий массив с 10 оценками вероятностей (в сумме дающих 1). Каждая оценка определяет вероятность принадлежности текущего изображения к одному из 10 классов цифр. Обученная модель обученная модель показала точность около 99%, с потерями 0.06. Наилучшее значение метрики было достигнуто на 17 эпохе и на валидационных данных показала точность в 99%, с потерями 0.08.\n",
        "\n",
        "2. На втором этапе мы классифицировали изображений кошек и собак из набора данных Cats vs. Dogs. Создали генератор ImageDataGenerator с несколькими параметрами для аугментации данных и предварительной обработки изображений. ImageDataGenerator реализует подход создания дополнительных обучающих данных из имеющихся путем трансформации образцов множеством случайных преобразований, дающих правдоподобные изображения. Сконструировали сверточную нейронную сеть (CNN).  Сверточная часть сети состоит из слоев _Conv2D_ и _MaxPooling2D_. На входе размер изображения _150 x 150_. Количество карт признаков на первом слое _Conv2D_ 64, размер ядра свертки 3x3, с функцией активацией _relu_ после каждого слоя _Conv2D_ применяем _MaxPooling2D_, уменьшая размер карты признаков вдвое. На следующих слоях применили тот же стек 2 раза, только изменяя количество карт признаков на слоях _Conv2D_ с 64 на 128. Добавили классификатор поверх сверточной нейронной сети. Первый слой состоит из 1024 нейронов, с функцией активацией _relu_. Последний слой состоит из 1 нейрона с функцией активации _sigmoid_, на выходе получая скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Обученная модель достигла точности около 83%. Снижение точности связано с неоптимальной архитектурой нейронной сети и небольшим количеством обучающих данных. Наилучшее значение метрики было достигнуто на 9 эпохе, и обученная модель показала точность около 90%, с потерями 1.1. \n",
        "\n",
        "3. В третьем этапе была таже задача, что и на втором этапе, но был использован метод выделения признаков с помощью сверточной нейронной сети VGG-16. Полученные данные затем подавались на многослойную полносвязную нейронную сеть с двумя слоями. Первый слой состоит из 1024 нейронов, с функцией активацией _relu_. Последний слой состоит из 1 нейрона с функцией активации _sigmoid_, на выходе получаем скалярное значение в диапозоне между 0 и 1, представляющее собой вероятность. Для преодоления проблемы переобучения, как один из наиболее эффективных и распространенных приемов регуляризации для нейронных сетей, применили метод Dropout, с коэффициентом прореживания 0.5. Наилучшее значение метрики было достигнуто на 9 эпохе, и обученная модель показала точность около 77%, с потерями 0.49.\n",
        "\n",
        "<p style=\"text-align: center;\">Список использованной литературы</p>\n",
        "\n",
        "1. Шолле Франсуа. Глубокое обучение на Python. - СПб.: Питер, 2018. - 400 с.: ил. - (Серия «Библиотека программиста»).\n",
        "2. Николенко С., Кадурин А., Архангельская Е. Глубокое обучение. — СПб.: Питер, 2018. — 480 с.: ил. — (Серия «Библиотека программиста»)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
